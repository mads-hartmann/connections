{
  "data": [
    {
      "id": 218332,
      "feed_id": 43,
      "title": "AoAH Day 25: Claude OCaml Marketplace for all your festive coding needs",
      "url": "https://anil.recoil.org/notes/aoah-2025-25",
      "published_at": "2025-12-25 00:00:00",
      "content": "<p>I'm somewhat frazzled after managing to get through <a href=\"/notes/aoah-2025\">25 days of agentic coding</a>.  It hasn't actually been that much physical work, but I\nunderestimated just how much my brain would be in full gear multitasking across\n<em>so many</em> terminal windows and ideas.  My outbound queue that I didn't manage to write\nup is enormous: I have sessions running with OxCaml experiments, io_uring webservers, implementations of ATProto, a pure OCaml Parquet, and some even stranger ideas I won't go into now!</p>\n<p>In the past, my computer systems brain was limited by the speed of coding, but now it feels like we're entering a different age. I'll reserve my longform thoughts on all of this for the new year as I need to head into Christmas festivities, but I wanted to leave you all with my <strong><a href=\"https://github.com/avsm/ocaml-claude-marketplace\">Claude Code OCaml marketplace</a></strong> in case you want to try this stuff for yourself!</p>\n<p><a href=\"https://github.com/avsm/ocaml-claude-marketplac\"> <img src=\"/images/aoah-plugin-ss-1.webp\" alt=\"%c\" > </a></p>\n<h2 id=\"claude-skills-and-marketplaces\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#claude-skills-and-marketplaces\"></a>Claude Skills and marketplaces</h2>\n<p>Claude has made it very simple to distribute <a href=\"https://code.claude.com/docs/en/plugin-marketplaces\">plugins</a>. I just created a GitHub <a href=\"https://github.com/avsm/ocaml-claude-marketplace\">avsm/ocaml-claude-marketplace</a> and kicked off Claude's own development plugin to develop plugins with. I then fed it all my skills and asked it to generalise them a bit.  They'll likely need some work to generalise beyond me, but feel free to send in PRs!</p>\n<p><img src=\"/images/aoah-plugin-ss-4.webp\" alt=\"%c\" title=\"Just go to the marketplaces section under /plugins in Claude\" ></p>\n<p><img src=\"/images/aoah-plugin-ss-2.webp\" alt=\"%c\" title=\"It'll auto install it and you can find all the plugins for OCaml\" ></p>\n<p><img src=\"/images/aoah-plugin-ss-3.webp\" alt=\"%c\" title=\"And you can enable or disable it selectively\" ></p>\n<p>I'll be back in the new year with more thoughts, but in the meanwhile, I hope\nyou all have a very good holiday and a chance to recharge! It's going to be a\nmad 2026...</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-25 13:11:41",
      "read_at": null,
      "tags": []
    },
    {
      "id": 195788,
      "feed_id": 28,
      "title": "uv-init-demos",
      "url": "https://simonwillison.net/2025/Dec/24/uv-init-demos/#atom-everything",
      "published_at": "2025-12-24 22:05:23",
      "content": "\n    \n<p><strong><a href=\"https://github.com/simonw/uv-init-demos\">uv-init-demos</a></strong></p>\n<code>uv</code> has a useful <code>uv init</code> command for setting up new Python projects, but it comes with a bunch of different options like <code>--app</code> and <code>--package</code> and <code>--lib</code> and I wasn't sure how they differed.</p>\n<p>So I created this GitHub repository which demonstrates all of those options, generated using this <a href=\"https://github.com/simonw/uv-init-demos/blob/main/update-projects.sh\">update-projects.sh</a> script (<a href=\"https://gistpreview.github.io/?9cff2d3b24ba3d5f423b34abc57aec13\">thanks, Claude</a>) which will run on a schedule via GitHub Actions to capture any changes made by future releases of <code>uv</code>.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/projects\">projects</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/github-actions\">github-actions</a>, <a href=\"https://simonwillison.net/tags/git-scraping\">git-scraping</a>, <a href=\"https://simonwillison.net/tags/uv\">uv</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-24 22:43:47",
      "read_at": null,
      "tags": []
    },
    {
      "id": 218333,
      "feed_id": 43,
      "title": "AoAH Day 24: Tuatara, an evolving Atom aggregator that mutates",
      "url": "https://anil.recoil.org/notes/aoah-2025-24",
      "published_at": "2025-12-24 00:00:00",
      "content": "<p>My original purpose for starting this <a href=\"/notes/aoah-2025\">AoAH</a> series was to build a\nfeed aggregator for my group website, so I had to finish up with something to show!</p>\n<p>I'm not sure if taking the <a href=\"https://www.goodreads.com/quotes/6001-think-you-re-escaping-and-run-into-yourself-longest-way-round\">longest way around</a>\nwas wise here but I ended up building <strong><a href=\"https://tangled.org/anil.recoil.org/tuatara\">tuatara</a></strong>, an\naggregator to pull together all my colleagues' writing into one place.\nThey're a quirky bunch with many diverse homegrown feeds in various\nstates of brokenness, so it's difficult to build a one-size-fits-all tool.</p>\n<p>So given it's the end of the year and I'm sozzled on Christmas eve on\nmulled wine, I decided to make Tuatara <strong>mutate its own code</strong> by linking with my <a href=\"/notes/aoah-2025-4\">Claudeio</a> library to\nforce it to evolve and modify itself as it runs across feed errors. Every deployment of\nTuatara is meant to be slightly <a href=\"/papers/2025-internet-ecology\">different</a>.</p>\n<h2 id=\"evolving-code-like-its-2026\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#evolving-code-like-its-2026\"></a>Evolving code like it's 2026</h2>\n<p>The initial generation of the code was pretty straightforward, using Sqlite to\nstore a database with all the posts and importing metadata from my previously\ncreated <a href=\"/notes/aoah-2025-8\">Sortal</a> contacts manager.</p>\n<pre><code class=\"language-bash\">&gt; tuatara import-sortal\nSortal Import Results:\n\n  Total contacts scanned: 420\n  Contacts with feeds: 15\n  Feeds imported: 16\n  Feeds skipped (already exist): 0\n\nRun 'tuatara fetch' to download posts from the imported feeds.\n</code></pre>\n<p>But when we actually get the feeds, I rapidly realised that there are lots of\nparsing quirks needed:</p>\n<pre><code class=\"language-bash\">&gt; tuatara fetch\nFetching Anil Madhavapeddy...\n  340 posts (0 new)\nFetching David Allsopp...\n  Not modified\nFetching Jessica Man...\n  Not modified\nFetching Jon Ludlam...\n  28 posts (0 new)\nFetching Jon Sterling...\n  Not modified\nFetching Mark Elvers...\n  Not modified\nFetching Martin Kleppmann...\n  Error: Feed parse error: document MUST contains exactly one &lt;feed&gt; element at l.0 c.0\n  URL: http://feeds.feedburner.com/martinkl\nFetching Onkar Gulati...\n  Error: Not_found\n  URL: https://onkargulati.com/feed.xml\nFetching Patrick Ferris...\n  Error: Feed parse error: &lt;entry&gt; elements MUST contains at least an &lt;author&gt; element or &lt;feed&gt; element MUST contains one or more &lt;author&gt; elements at l.1460 c.7\n  URL: http://patrick.sirref.org/weeklies/atom.xml\nFetching Richard Mortier...\n  79 posts (79 new)\nFetching Ryan Gibb...\n  38 posts (38 new)\nFetching Sadiq Jaffer...\n  10 posts (10 new)\n\nTotal: 127 new posts (3 errors)\n</code></pre>\n<p>Either we skip content, or talk to the people involved to fix their feeds, but\nit's Christmas eve so that's unlikely. And anyway, we want to be <a href=\"https://en.wikipedia.org/wiki/Robustness_principle\">liberal in\nwhat we accept</a> so why\ncan't I fix my own software first?!</p>\n<p><img src=\"/images/aoah-tuatara-ss-1.webp\" alt=\"%c\" title=\"Like the amazing Tuatara, why don't we build evolution directly into our software? Time to take off the semantics seatbelt...\" ></p>\n<h2 id=\"medice-cura-te-ipsum\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#medice-cura-te-ipsum\"></a>Medice, cura te ipsum</h2>\n<p>The non-obvious and probably-terrible answer here is to use our fancy coding\nmodels to force the Tuatara source code to heal itself.  I added an <code>--evolve</code>\nflag to allow tuatara to invoke <a href=\"/notes/aoah-2025-4\">Claude/OCaml</a> upon errors and patch its own code.\nTurns out self help works, as Martin's blog feed was just fine!</p>\n<pre><code class=\"language-bash\">Fetching Martin Kleppmann...\n  Error: Feed parse error: document MUST contains exactly one &lt;feed&gt; element at l.0 c.0\n  URL: http://feeds.feedburner.com/martinkl\n\nInvoking Claude Code to fix parse error...\n\nThe feed from `http://feeds.feedburner.com/martinkl` is an **RSS 2.0 feed** (it\nstarts with `&lt;rss version=&quot;2.0&quot;&gt;`), but tuatara was incorrectly detecting it as\nan **Atom feed**. This caused the Syndic Atom parser to fail with &quot;document\nMUST contains exactly one \\&lt;feed\\&gt; element&quot;.\n\nThe root cause was that the `detect_feed_type` function prioritized the HTTP\n`Content-Type` header over the actual content. FeedBurner (and other feed\naggregators) often serve RSS feeds with an incorrect `application/atom+xml`\ncontent-type header.\n\nThis is a generic fix that will work for any feed aggregator or CDN that\nmis-labels RSS feeds as Atom (or vice versa), so no domain-specific quirk was\nneeded.\n</code></pre>\n<p>But the parsing drama continued, as <a href=\"https://mynameismwd.org\">Michael Dales</a> uses the wrong date format in his\nfeed (tsk tsk I'd send the RFC police out if it wasn't Christmas), but Tuatara\n<a href=\"https://tangled.org/anil.recoil.org/tuatara/commit/a56c4a877936a1ae49c0fab301fb6afbcc74b3bb\">evolves a quirk</a> that gets past it:</p>\n<blockquote>\n<p>The quirk module converts ISO 8601 dates (2025-10-22T12:24:00-00:00) to RFC\n822 format (Wed, 22 Oct 2025 12:24:00 GMT) which is what Syndic's RSS2 parser\nexpects.</p>\n</blockquote>\n<p>And <a href=\"https://www.cst.cam.ac.uk/people/og309\">Onkar Gulati</a> and <a href=\"https://patrick.sirref.org\">Patrick Ferris</a> both have an empty author field which would\nordinarily give us a dreaded <code>Not_found</code> exception:</p>\n<blockquote>\n<p>Fetching Patrick Ferris...Error: Feed parse error: <entry> elements MUST contains at least an <author> element or <feed> element MUST contains one or more <author> elements at l.1460 c.7 URL: http://patrick.sirref.org/weeklies/atom.xml</p>\n</blockquote>\n<p>But never fear, the inexorable <code>--evolve</code> flag figures it out and patches its own code!</p>\n<p>There were some non-trivial quirks as well; <a href=\"https://ancazugo.github.io/\">Andres Zuñiga-Gonzalez</a> uses Quatro for his website which puts the entire HTML blob into the summary field, but the evolution managed to use <a href=\"/notes/aoah-2025-15\">html5rw</a> to parse <a href=\"https://tangled.org/anil.recoil.org/tuatara/commit/7f29b37e1c647f984589e42164a0fc2ec0cda5c4\">its way out of this</a>. This sort of fix is very hard to generalise, so it's actually quite useful for the tool to fix itself on demand for our small group.</p>\n<h2 id=\"using-the-claude-frontend-design\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#using-the-claude-frontend-design\"></a>Using the Claude frontend design</h2>\n<p>Then I needed a quick way to do a clean frontend output so I can visualise the\nJSONfeed. Claude has a <code>/plugin frontend-design</code> skill that is built in, and\nprompting it to give me a few designs let me integrate a <code>--html</code> output.</p>\n<p>And because it's Christmas, I added some snowflakes as well. Yay!</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/eeg-xmas\"> <img src=\"/images/aoah-tuatara-ss-4.webp\" alt=\"%c\" title=\"Ho ho ho merry xmas everyone from the EEG feed that isnt live yet but will be after the new year\" > </a></p>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p>The paper I enjoyed writing the most this year was <a href=\"/papers/2025-internet-ecology\">Steps towards an Ecology for the Internet</a> for\n<a href=\"/notes/ecology-at-aarhus\">Aarhus 2025</a>. In the back of my head since has been a desire\nto start figuring out what self-evolving software actually might be. It's a\nstrange, and probably impractical idea, but I'm delighted that I took a tiny\nstep towards it with this project.</p>\n<p>Back in March, I had the honour of being invited to a <a href=\"https://bellairs.net\">Bellairs</a> meeting to discuss a heady combination of semantics and computational science. <a href=\"https://jonmsterling.com\">Jon Sterling</a> demonstrated his wonderfully organised Forester website. And I... showed how my mismash of semi-structured writings can kind of be connected together in a vaguely coherent way to build my website. Next year will have me thinking much harder about the implications of <a href=\"/papers/2025-internet-ecology\">self-evolving code</a>, of how radically <a href=\"/papers/2025-biodiversity-9recs\">transformative to global biodiversity</a> semi-structured agentic processing might be, and other heavy matters. But to close this year, I'm disproportionately pleased to have gotten my tiny website under control a little!</p>\n<p><img src=\"/images/aoah-tuatara-ss-2.webp\" alt=\"%c\" title=\"Sitting indoors in Barbados with a gigantic beach outside: a classic sign of semanticists in the wild\" ></p>\n<p>As I noted in my <a href=\"/notes/acm-ai-recs\">letter to the ACM</a>, it's important that we can use AI for things that boost the\nhuman condition; I really enjoy reading my colleagues' long form thoughts much\nmore than doomscrolling on the web, and so making it easier to gather their\nthoughts digestibly and easily is a nice end to my <a href=\"\">agentic humps</a>\neffort. Tomorrow on <a href=\"/notes/aoah-2025-25\">Christmas</a> I'll publish all the skills I used so others can try them out.</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-25 13:11:41",
      "read_at": null,
      "tags": []
    },
    {
      "id": 167239,
      "feed_id": 28,
      "title": "Quoting Salvatore Sanfilippo",
      "url": "https://simonwillison.net/2025/Dec/23/salvatore-sanfilippo/#atom-everything",
      "published_at": "2025-12-23 23:03:00",
      "content": "\n    <blockquote cite=\"https://news.ycombinator.com/item?id=46367224#46368706\"><p>If this [MicroQuickJS] had been available in 2010, Redis scripting would have been JavaScript and not Lua. Lua was chosen based on the implementation requirements, not on the language ones... (small, fast, ANSI-C). I appreciate certain ideas in Lua, and people love it, but I was never able to <em>like</em> Lua, because it departs from a more Algol-like syntax and semantics without good reasons, for my taste. This creates friction for newcomers. I love friction when it opens new useful ideas and abstractions that are worth it, if you learn SmallTalk or FORTH and for some time you are lost, it's part of how the languages are different. But I think for Lua this is not true enough: it feels like it departs from what people know without good reasons.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://news.ycombinator.com/item?id=46367224#46368706\">Salvatore Sanfilippo</a>, Hacker News comment on MicroQuickJS</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/salvatore-sanfilippo\">salvatore-sanfilippo</a>, <a href=\"https://simonwillison.net/tags/lua\">lua</a>, <a href=\"https://simonwillison.net/tags/redis\">redis</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-23 23:50:05",
      "read_at": null,
      "tags": []
    },
    {
      "id": 164069,
      "feed_id": 28,
      "title": "MicroQuickJS",
      "url": "https://simonwillison.net/2025/Dec/23/microquickjs/#atom-everything",
      "published_at": "2025-12-23 20:53:40",
      "content": "\n    \n<p><strong><a href=\"https://github.com/bellard/mquickjs\">MicroQuickJS</a></strong></p>\nNew project from programming legend Fabrice Bellard, of ffmpeg and QEMU and QuickJS and <a href=\"https://bellard.org\">so much more</a> fame:</p>\n<blockquote>\n<p>MicroQuickJS (aka. MQuickJS) is a Javascript engine targetted at embedded systems. It compiles and runs Javascript programs with as low as 10 kB of RAM. The whole engine requires about 100 kB of ROM (ARM Thumb-2 code) including the C library. The speed is comparable to QuickJS.</p>\n</blockquote>\n<p>It supports <a href=\"https://github.com/bellard/mquickjs/blob/17ce6fe54c1ea4f500f26636bd22058fce2ce61a/README.md#javascript-subset-reference\">a subset of full JavaScript</a>, though it looks like a rich and full-featured subset to me.</p>\n<p>One of my ongoing interests is sandboxing: mechanisms for executing untrusted code - from end users or generated by LLMs - in an environment that restricts memory usage and applies a strict time limit and restricts file or network access. Could MicroQuickJS be useful in that context?</p>\n<p>I fired up Claude Code for web (on my iPhone) and kicked off <a href=\"https://simonwillison.net/2025/Nov/6/async-code-research/\">an asynchronous research project</a> to see explore that question:</p>\n<p>My full prompt <a href=\"https://github.com/simonw/research/pull/50#issue-3757781692\">is here</a>. It started like this:</p>\n<blockquote>\n<p><code>Clone https://github.com/bellard/mquickjs to /tmp</code></p>\n<p><code>Investigate this code as the basis for a safe sandboxing environment for running untrusted code such that it cannot exhaust memory or CPU or access files or the network</code></p>\n<p><code>First try building python bindings for this using FFI - write a script that builds these by checking out the code to /tmp and building against that, to avoid copying the C code in this repo permanently. Write and execute tests with pytest to exercise it as a sandbox</code></p>\n<p><code>Then build a \"real\" Python extension not using FFI and experiment with that</code></p>\n<p><code>Then try compiling the C to WebAssembly and exercising it via both node.js and Deno, with a similar suite of tests [...]</code></p>\n</blockquote>\n<p>I later added to the interactive session:</p>\n<blockquote>\n<p><code>Does it have a regex engine that might allow a resource exhaustion attack from an expensive regex?</code></p>\n</blockquote>\n<p>(The answer was no - the regex engine calls the interrupt handler even during pathological expression backtracking, meaning that any configured time limit should still hold.)</p>\n<p>Here's <a href=\"https://gistpreview.github.io/?6e07c54db7bb8ed8aa0eccfe4a384679\">the full transcript</a> and the <a href=\"https://github.com/simonw/research/blob/main/mquickjs-sandbox/README.md\">final report</a>.</p>\n<p>Some key observations:</p>\n<ul>\n<li>MicroQuickJS is <em>very</em> well suited to the sandbox problem. It has robust near and time limits baked in, it doesn't expose any dangerous primitive like filesystem of network access and even has a regular expression engine that protects against exhaustion attacks (provided you configure a time limit).</li>\n<li>Claude span up and tested a Python library that calls a MicroQuickJS shared library (involving a little bit of extra C), a compiled a Python binding and a library that uses the original MicroQuickJS CLI tool. All of those approaches work well.</li>\n<li>Compiling to WebAssembly was a little harder. It got a version working in Node.js and Deno and Pyodide, but the Python libraries wasmer and wasmtime proved harder, apparently because \"mquickjs uses setjmp/longjmp for error handling\". It managed to get to a working wasmtime version with <a href=\"https://github.com/simonw/research/blob/main/mquickjs-sandbox/README.md#working-solution\">a gross hack</a>.</li>\n</ul>\n<p>I'm really excited about this. MicroQuickJS is tiny, full featured, looks robust and comes from excellent pedigree. I think this makes for a very solid new entrant in the quest for a robust sandbox.\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/c\">c</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/nodejs\">nodejs</a>, <a href=\"https://simonwillison.net/tags/python\">python</a>, <a href=\"https://simonwillison.net/tags/sandboxing\">sandboxing</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/webassembly\">webassembly</a>, <a href=\"https://simonwillison.net/tags/deno\">deno</a>, <a href=\"https://simonwillison.net/tags/pyodide\">pyodide</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/claude-code\">claude-code</a>, <a href=\"https://simonwillison.net/tags/fabrice-bellard\">fabrice-bellard</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-23 21:13:46",
      "read_at": null,
      "tags": []
    },
    {
      "id": 132369,
      "feed_id": 28,
      "title": "Cooking with Claude",
      "url": "https://simonwillison.net/2025/Dec/23/cooking-with-claude/#atom-everything",
      "published_at": "2025-12-23 05:01:34",
      "content": "\n    <p>I've been having an absurd amount of fun recently using LLMs for cooking. I started out using them for basic recipes, but as I've grown more confident in their culinary abilities I've leaned into them for more advanced tasks. Today I tried something new: having Claude vibe-code up a custom application to help with the timing for a complicated meal preparation. It worked really well!</p>\n<h4 id=\"a-custom-timing-app-for-two-recipes-at-once\">A custom timing app for two recipes at once</h4>\n<p>We have family staying at the moment, which means cooking for four. We subscribe to a meal delivery service called <a href=\"https://www.greenchef.com/\">Green Chef</a>, mainly because it takes the thinking out of cooking three times a week: grab a bag from the fridge, follow the instructions, eat.</p>\n<p>Each bag serves two portions, so cooking for four means preparing two bags at once.</p>\n<p>I have done this a few times now and it is always a mad flurry of pans and ingredients and timers and desperately trying to figure out what should happen when and how to get both recipes finished at the same time. It's fun but it's also chaotic and error-prone.</p>\n<p>This time I decided to try something different, and potentially even more chaotic and error-prone: I outsourced the planning entirely to Claude.</p>\n<p>I took this single photo of the two recipe cards side-by-side and fed it to Claude Opus 4.5 (in the Claude iPhone app) with this prompt:</p>\n<blockquote>\n<p>Extract both of these recipes in as much detail as possible</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/recipe-cards.jpg\" alt=\"Two recipe cards placed next to each other on a kitchen counter. Each card has detailed instructions plus photographs of steps.\" style=\"max-width: 100%;\" /></p>\n</blockquote>\n<p>This is a moderately challenging vision task in that there quite a lot of small text in the photo. I wasn't confident Opus could handle it.</p>\n<p>I hadn't read the recipe cards myself. The responsible thing to do here would be a thorough review or at least a spot-check - I chose to keep things chaotic and didn't do any more than quickly eyeball the result.</p>\n<p>I asked what pots I'd need:</p>\n<blockquote>\n<p>Give me a full list of pots I would need if I was cooking both of them at once</p>\n</blockquote>\n<p>Then I prompted it to build a custom application to help me with the cooking process itself:</p>\n<blockquote>\n<p>I am going to cook them both at the same time. Build me a no react, mobile, friendly, interactive, artifact that spells out the process with exact timing on when everything needs to happen have a start setting at the top, which starts a timer and persists when I hit start in localStorage in case the page reloads. The next steps should show prominently with countdowns to when they open. The full combined timeline should be shown slow with calculated times tor when each thing should happen</p>\n</blockquote>\n<p>I copied the result out onto my own hosting (<a href=\"https://tools.simonwillison.net/blackened-cauliflower-and-turkish-style-stew\">you can try it here</a>) because I wasn't sure if localStorage would work inside the Claude app and I <em>really</em> didn't want it to forget my times!</p>\n<p>Then I clicked \"start cooking\"!</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/recipe-timer.gif\" alt=\"The recipe app shows a full timeline with 00:00 Preheat Oven and onwards, plus a big Start Cooking button. In the animation clicking the button starts a timer clicking up, adds a Do this now panel showing the Start all prep work step, shows Coming Up Next with timers counting down to the next steps and updates the full timeline to show local clock times where it previously showed durations from 00:00 upwards.\" style=\"max-width: 100%;\" /></p>\n<p>Here's the <a href=\"https://claude.ai/share/4acab994-c22b-4ddf-81bd-2f22d947c521\">full Claude transcript</a>.</p>\n<p>There was just one notable catch: our dog, Cleo, knows <em>exactly</em> when her dinner time is, at 6pm sharp. I forgot to mention this to Claude, which had scheduled several key steps colliding with Cleo's meal. I got woofed at. I deserved it.</p>\n<p>To my great surprise, <em>it worked</em>. I followed the recipe guide to the minute and served up both meals exactly 44 minutes after I started cooking.</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/recipe-finished.jpg\" alt=\"A small bowl (a beautiful blue sea textured bowl, made by Natalie Downe) contains a chickpea stew. A larger black bowl has couscous, green beans and blackened cauliflower.\" style=\"max-width: 100%;\" /></p>\n<p>The best way to learn the capabilities of LLMs is to throw tasks at them that may be beyond their abilities and see what happens. In this case I fully expected that something would get forgotten or a detail would be hallucinated and I'd end up scrambling to fix things half way through the process. I was surprised and impressed that it worked so well.</p>\n<p>Some credit for the app idea should go to my fellow hackers at <a href=\"https://devfort.com/fort/2/\">/dev/fort 2 in 2009</a>, when we rented Knockbrex Castle in Dumfries, Scotland for a week and attempted to build a cooking timer application for complex meals.</p>\n<h4 id=\"generating-recipes-from-scratch\">Generating recipes from scratch</h4>\n<p>Most of my other cooking experiments with LLMs have been a whole lot simpler than this: I ask for a recipe, ask for some variations and then cook one of them and see what happens.</p>\n<p>This works remarkably well considering LLMs have no taste buds.</p>\n<p>I've started to think of this as asking LLMs for the <em>average</em> recipe for a dish, based on all of the recipes they have hoovered up during their training. It turns out the mean version of every guacamole recipe on the internet is a decent guacamole!</p>\n<p>Here's an example of a recipe I tried recently that worked out really well. I was helping Natalie run her ceramic stall at the farmers market and the stall next to us sold excellent dried beans. I've never used dried beans before, so I took a photo of their selection and asked Claude what I could do with them:</p>\n<blockquote>\n<p><img src=\"https://static.simonwillison.net/static/2025/beans-1.jpg\" alt=\"Several bags of tasty looking beans of different varieties and colors\" style=\"width: 48%;\" /> <img src=\"https://static.simonwillison.net/static/2025/beans-2.jpg\" alt=\"More bags of beans.\" style=\"width: 48%;\" /></p>\n<p>Identify these beans</p>\n</blockquote>\n<p>It took a guess at the beans, then I said:</p>\n<blockquote>\n<p>Get me excited about cooking with these! If I bought two varietiew what could I make</p>\n</blockquote>\n<p>\"Get me excited\" switches Claude into a sort of hype-man mode, which is kind of entertaining:</p>\n<blockquote>\n<p>Oh, you're about to enter the <em>wonderful</em> world of bean cooking! Let me get you pumped about some killer two-bean combos: [...]</p>\n<p><strong>Mixed bean salad</strong> with lemon, olive oil, fresh herbs, cherry tomatoes - light but satisfying [...]</p>\n</blockquote>\n<p>I replied:</p>\n<blockquote>\n<p>OK Bean salad has me interested - these are dried beans. Give me some salad options I can make that would last a long time in the fridge</p>\n</blockquote>\n<p>... and after some back and forth we arrived on the <a href=\"https://claude.ai/share/c7534766-22d8-481b-bd80-a21abc53f5b2\">recipe in this transcript</a>, which I cooked the following day (asking plenty of follow-up questions) and thoroughly enjoyed.</p>\n<p>I've done this a bunch of times with a bunch of different recipes across both Claude and ChatGPT and honestly I've not had a notable miss yet. Being able to say \"make it vegan\" or \"I don't have coriander, what can I use instead?\" or just \"make it tastier\" is a really fun way to explore cooking.</p>\n<p>It's also fun to repeat \"make it tastier\" multiple times to see how absurd you can get.</p>\n<h4 id=\"i-really-want-someone-to-turn-this-into-a-benchmark-\">I really want someone to turn this into a benchmark!</h4>\n<p>Cooking with LLMs is a lot of fun. There's an opportunity here for a <em>really</em> neat benchmark: take a bunch of leading models, prompt them for recipes, follow those recipes and taste-test the results!</p>\n<p>The logistics of running this are definitely too much for me to handle myself. I have enough trouble cooking two meals at once, for a solid benchmark you'd ideally have several models serving meals up at the same time to a panel of tasters.</p>\n<p>If someone else wants to try this please let me know how it goes!</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/cooking\">cooking</a>, <a href=\"https://simonwillison.net/tags/devfort\">devfort</a>, <a href=\"https://simonwillison.net/tags/tools\">tools</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/vision-llms\">vision-llms</a>, <a href=\"https://simonwillison.net/tags/vibe-coding\">vibe-coding</a></p>\n    \n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-23 05:36:54",
      "read_at": "2025-12-23 18:03:00",
      "tags": []
    },
    {
      "id": 194527,
      "feed_id": 43,
      "title": "AoAH Day 23: Unpac unifies git branching with package management",
      "url": "https://anil.recoil.org/notes/aoah-2025-23",
      "published_at": "2025-12-23 00:00:00",
      "content": "<p>Yesterday's <a href=\"/notes/aoah-2025-22\">monopam</a> workflow used git submodules to combine\nvendored packages, but was awkward to use for crosscutting changes involving\nlots of vendored git repositories. Today I asked what agentic code development\nwould look like if we could unify <em>all code</em> into a single git repository,\nwhere upstream packages become branches instead of submodules. I've\nopen-sourced the <strong><a href=\"https://tangled.org/anil.recoil.org/unpac\">unpac</a></strong> CLI to\nexplore this, and have begun using it myself.</p>\n<p>Coding agents work best when all relevant code is locally available so\nthey can grep and make <a href=\"/notes/aoah-2025-21\">cross-cutting changes</a>. I first noticed this when building <a href=\"/notes/aoah-2025-9\">Bonsai terminal UIs</a> and <a href=\"/notes/aoah-2025-10\">Mosaic</a>, where I had to manually assemble monorepos just to get the agent working.\nThings come to a crashing halt when package management\ngets involved; the tool calls for web search are far slower and unreliable.\nThis means that the agent doesn't really have a good view on what third-party\npackages might be useful to solve a problem, leading to the common complaint\nthat LLMs <a href=\"https://ryan.freumh.org/claude-code.html\">reinvent the wheel</a>.</p>\n<p>To fix this, unpac parses package metadata and materialises it into a git branch\nstructure <em>in a single repository</em> to make vendoring, patching, and updating\na native git workflow. Local changes can later be exported into git patches for\nsending upstream, but in the meanwhile our agents can work on a single\nrepository.</p>\n<p>The secret sauce to working on so many branches is to use <a href=\"https://git-scm.com/docs/git-worktree\">git worktrees</a>, which allow multiple\nbranches to be checked out simultaneously from one git repo! I'll explain how unpac works next, and you can\nbrowse a <a href=\"https://tangled.org/anil.recoil.org/unpac-work\">working unpac tree</a>. You do end up with a\nlot of git branches, which got me <a href=\"https://x.com/rhatr/status/1012001138110029824\">banned from GitHub</a> back when I announced <a href=\"https://www.theregister.com/2017/01/17/docker_adds_continuous_integration_to_datakit/\">Docker DataKit</a>. Luckily this time around I am hosting on <a href=\"/notes/disentangling-git-with-bluesky\">Tangled</a> where I host my own Git remotes and so don't have to worry about third-party SLAs!</p>\n<p><a href=\"https://tangled.org/anil.recoil.org/unpac-work\"> <img src=\"/images/aoah-unpac-ss-6.webp\" alt=\"%c\" title=\"All the dependent code is in separate branches in the git repo, managed by unpac\" > </a></p>\n<h2 id=\"the-unpac-branching-model\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-unpac-branching-model\"></a>The unpac branching model</h2>\n<p>unpac organises code and dependencies using a lot of unrelated git branches, with\ncareful merging across them.  The <code>main</code> branch only holds the unpac metadata about which projects exist and which opam remotes to use.\nWhile this defaults to the <a href=\"https://github.com/ocaml/opam-repository\">upstream opam-repository</a>, I'm also using this with the <a href=\"https://github.com/oxcaml/opam-repository\">oxcaml/opam-repository</a> and my <a href=\"https://tangled.org/anil.recoil.org/aoah-opam-repo\">aoah-opam-repo</a> overlays (maintained via the <a href=\"/notes/aoah-2025-5\">opam metadata skill</a>) to help track community forks.</p>\n<pre><code>[opam]\ncompiler = &quot;5.4.0&quot;\n\n[[opam.repositories]]\nname = &quot;opam&quot;\npath = &quot;/workspace/opam/opam-repository&quot;\n\n[[opam.repositories]]\nname = &quot;aoah&quot;\npath = &quot;/workspace/opam/aoah-opam-repo&quot;\n</code></pre>\n<p>Each third-party opam package then has <em>three</em> branches in my unpac repo:</p>\n<ul>\n<li><code>upstream/opam/&lt;pkg&gt;</code> holds the unmodified upstream code and history</li>\n<li><code>vendor/opam/&lt;pkg&gt;</code> is upstream history relocated to a <code>vendor/opam/&lt;pkg&gt;/</code> prefix using <a href=\"https://github.com/newren/git-filter-repo\">git-filter-repo</a>.</li>\n<li><code>patches/opam/&lt;pkg&gt;</code> is the vendor branch with any local changes applied.</li>\n</ul>\n<p>The projects you are working on are all in a <code>project/&lt;name&gt;</code> orphan git\nbranches, independent of all the others.  Adding a dependency to a project is a mere matter of\n<em>merging</em> the vendor branch for a given dependency into the <code>project</code> branch.</p>\n<p>This merging will materialise the dependency code in the project branch\nconflict-free under <code>vendor/opam/</code>. This allows us to build a monorepo of OCaml\ncode that's maintained its git history across all the different developers,\nwhile also allowing local commits to be held and rebased. The agent has a ton\nof context available to it now without having to go to the outside world!</p>\n<h3 id=\"working-on-multiple-branches-simultaneously-with-worktrees\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#working-on-multiple-branches-simultaneously-with-worktrees\"></a>Working on multiple branches simultaneously with worktrees</h3>\n<p>Before showing off the unpac CLI, it's worth explaining git worktrees as I'd\nnever used them before today.  Normally, you can only have one git branch\nchecked out at a time, but worktrees free us of that restriction:</p>\n<blockquote>\n<p>A git repository can support multiple working trees, allowing you to check\nout more than one branch at a time. With <code>git worktree add</code> a new working\ntree is associated with the repository, along with additional metadata that\ndifferentiates that working tree from others in the same repository. The\nworking tree, along with this metadata, is called a &quot;worktree&quot;.</p>\n<p>This new worktree is called a &quot;linked worktree&quot; as opposed to the &quot;main\nworktree&quot; prepared by git-init or git-clone. A repository has one main\nworktree (if it's not a bare repository) and zero or more linked worktrees.\nWhen you are done with a linked worktree, remove it with <code>git worktree remove</code>.\n<cite>-- <a href=\"https://git-scm.com/docs/git-worktree\">git-worktree documentation</a>, 2023</cite></p>\n</blockquote>\n<p>Creating them is pretty straightforward using <code>git worktree add</code>.  This creates\na new checkout with the <code>.git</code> entry being a file containing an entry\nlike:</p>\n<pre><code>gitdir: /workspace/git/worktrees/tuatara\n</code></pre>\n<p>Without worktrees, agents fall over themselves switching branches due to the\nrequirement that all files be committed before switching. With worktrees, you\ncan have uncommitted stuff in multiple branches, meaning we can simultaneously\nview upstream code while making patches, compare vendor and patches branches\nside-by-side with diff, and work on multiple projects from the same repository.\nI used to have these affordances back when I used Mercurial and Perforce (with\ndifferent distribution models, admittedly), so it's great to have it back!</p>\n<h2 id=\"using-unpac-via-the-cli\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#using-unpac-via-the-cli\"></a>Using unpac via the CLI</h2>\n<p>This elaborate git schema is all very good, but not something I'd want to manage\nmanually. The unpac CLI takes care of all the gruntwork involved,\nincluding integrating an opam solver to create 100s of branches with one\ncommand. Let's take a look at an example project:</p>\n<pre><code class=\"language-bash\">$ unpac init\n\n# Vendors in packages from opam including dependencies\n$ unpac add opam eio --solve\n</code></pre>\n<p>We now have a bunch of vendor branches in our local repository, and need to\ncreate a project to use them:</p>\n<pre><code class=\"language-bash\"># Create a new project branch\n$ unpac project new myapp\n\n# Merge in the patch branches of these dependencies into project/myapp/vendor\n$ unpac opam merge eio --solve myapp\n\n# Hax0r like it's 2026 on your project\n$ cd project/myapp\n</code></pre>\n<p><img src=\"/images/aoah-unpac-ss-1.webp\" alt=\"%c\" title=\"The unpac CLI solves package constraints and merges individual branches into a project\" ></p>\n<h2 id=\"doing-agentic-monorepo-development\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#doing-agentic-monorepo-development\"></a>Doing agentic monorepo development</h2>\n<p>We can now do a simple <code>dune build</code> in the <code>project/myapp</code> directory since all our code is present in one working branch.\nA typical unpac project looks like:</p>\n<pre><code>.\n└── dune-project\n├── vendor/\n│   ├── eio/            # vendored eio source\n│   ├── lwt/            # vendored lwt source\n│   └── ...\n├── src/                # your project source\n</code></pre>\n<p>Dune <a href=\"/notes/aoah-2025-22\">automatically discovers and builds</a> the vendored packages. No special configuration is needed beyond standard dune files.</p>\n<p>However, some packages don't build with dune since the upstream projects don't use dune but have other build systems (quite reasonably! Choice is important). In the past, I have maintained over 50 <a href=\"https://github.com/dune-universe\">dune ports</a> via opam overlays, mostly by hand. However, I can now easily use my coding agent to do all the porting automatically.</p>\n<p><img src=\"/images/aoah-unpac-ss-4.webp\" alt=\"%c\" title=\"Claude can spawn parallel subagents using git worktrees to do the ports independently.\" ></p>\n<p>You can see some of the diffs in the patch branches in my working tree: <a href=\"https://tangled.org/anil.recoil.org/unpac-work/tree/opam%2Fpatches%2Flogs\">patches/logs</a> or <a href=\"https://tangled.org/anil.recoil.org/unpac-work/tree/opam%2Fpatches%2Fcmdliner\">patches/cmdliner</a> or <a href=\"https://tangled.org/anil.recoil.org/unpac-work/tree/opam%2Fpatches%2Fbos\">patches/bos</a> for example. Since the agent has a clean local interface to work with, it can keep its commits neatly organised.</p>\n<p>An <code>unpac vendor status</code> command neatly summarises the status of which packages have been patched, and which project they're merged into:</p>\n<pre><code class=\"language-bash\">$ unpac vendor status\nPackage                    Patches   Merged into\n----------------------------------------------------------------------\nangstrom                         0   -\nasn1-combinators                 0   myapp\nastring                          0   -\nbase64                           0   myapp\nbigstringaf                      0   -\nbos                              1   -\nbytesrw                          0   -\nbytesrw-eio                      0   -\nca-certs                         0   -\ncheckseum                        0   -\ncmdliner                         1   tuatara\nconpool                          0   -\ncookeio                          0   -\ncsexp                            0   myapp\ncstruct                          0   -\ndecompress                       0   -\ndigestif                         0   myapp, tuatara\ndomain-local-await               0   -\ndomain-name                      0   myapp\n</code></pre>\n<p>You can see here that <code>cmdliner</code> has been patched and merged into the tuatara\nproject, but <code>bos</code> has been patched and is unmerged. This is all calculated\ninternally via git commands, so there's no separate metadata store to get out\nof sync.</p>\n<p>I haven't completed porting all the third-party packages to use dune just yet,\nbut I've left it running overnight. When that's done, the big feature we gain\nis that a dune build can seamlessly cross-compile binaries since all the OCaml\ncode and C bindings are in one place. This is what <a href=\"https://mirage.io/docs/mirage-4\">MirageOS\n4</a> does, and we can reap the benefits now for\nconventional binaries too. Windows builds should also be a lot easier as long\nas the dune rules don't have too many Unixisms.</p>\n<h3 id=\"importing-existing-projects\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#importing-existing-projects\"></a>Importing existing projects</h3>\n<p>The unpac CLI was self-explaining enough that another agent session could import\nan OCaml project by analysing that project and running the sequence of unpac\ncommands.</p>\n<p><img src=\"/images/aoah-unpac-ss-2.webp\" alt=\"%c\" title=\"Importing and vendoring all the code needed for an existing project using Claude\" ></p>\n<p>In order to reduce the load on external git clones, unpac also supports having a local &quot;git branch cache&quot; which pulls remotes just once, and then all unpac invocations pull from that local store. As an experiment over the holidays, I've left a session doing a slow clone of <em>all</em> opam git remotes, to see how well git scales to a few thousand branches.</p>\n<h3 id=\"pushing-the-results\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#pushing-the-results\"></a>Pushing the results</h3>\n<p>We do end up with 100s of local branches, and so an <code>unpac push</code> command checks which ones need pushing and takes care of it for you.</p>\n<p><img src=\"/images/aoah-unpac-ss-3.webp\" alt=\"%c\" ></p>\n<p>You can browse one of my working unpac repositories on\n<a href=\"https://tangled.org/anil.recoil.org/unpac-work\">tangled/anil.recoil.org/unpac-work</a>\nto get a sense of the structure.</p>\n<p>I'm still working on the pulling/rebasing functionality, but the basic idea is\nall the same: pull from the outside world into a pristine branch, relocate the\ndirectory, and then have local patch branches.</p>\n<h2 id=\"integrating-ocaml-with-other-languages-in-one-repo\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#integrating-ocaml-with-other-languages-in-one-repo\"></a>Integrating OCaml with other languages in one repo</h2>\n<p>The current unpac focuses on opam, but <a href=\"https://ryan.freumh.org\">Ryan Gibb</a> has been leading the research on a <a href=\"/papers/2025-hyperres\">generalised packaging language</a> that can describe package management <em>across</em> ecosystems. Imagine something like this in a future unpac:</p>\n<pre><code class=\"language-toml\"># Works with opam, npm, cargo, pip...\ndependencies = [\n  { source = &quot;opam&quot;, name = &quot;eio&quot;, version = &quot;&gt;=1.0&quot; },\n  { source = &quot;npm&quot;, name = &quot;d3&quot;, version = &quot;^7.0&quot; },\n  { source = &quot;cargo&quot;, name = &quot;tokio&quot;, version = &quot;1.0&quot; },\n]\n</code></pre>\n<p>I've already had need for this last week when I <a href=\"/notes/aoah-2025-13\">vibespiled 50 HTTP libraries</a> across 10 languages into an OCaml implementation. I really want to be able to more easily draw from other language ecosystems, and unpac's git branch model works regardless of the package manager (hence the <code>opam/</code> suffix for <code>vendor/</code> branches).</p>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p><a href=\"https://x.com/rhatr/status/1012001138110029824\"> <img src=\"/images/aoah-unpac-ss-5.webp\" alt=\"%rc\" title=\"I did not get banned from anything while writing this post\" > </a></p>\n<p>Unpac's branching model actually doesn't work hugely well with GitHub due to the storage limits on an account being hit pretty fast, but it's peachy when used with self-hosted Git services. I'm sure we could do something with Git object alternates as well to improve on this in the future.</p>\n<p>There's quite a lot of work required to make unpac production grade, but I'm astounded by how quickly I could put this prototype together in a day. Sketching out CLI tools and cram tests is extraordinarily fun as well, as I could specify my desired user interface and then engage in a Socratic dialogue with the agent to refine the specification.</p>\n<p>I'm also having subversive thoughts now about issue management. I've been a fan for many years of <a href=\"https://github.com/janestreet/iron\">Jane Street's Iron</a> code review system. However, despite having talked to Stephen Weeks and <a href=\"https://github.com/yminsky\">Yaron Minsky</a> extensively about it over the years, I've never found the bandwidth to build an equivalent for open source. But with coding agents being able to interpret natural language alongside code, it seems like a really obvious extension to also store issues within branches as well as code, and to unify our agent context horizon. Something for the 2026 queue!</p>\n<p>I'd love to hear any feedback on unpac's model from other projects. I wouldn't use the tool I've released just yet as it's only about 18 hours old, but I'll work on it more in the new year as well and do a proper release once it's self hosting. Many thanks to <a href=\"https://ryan.freumh.org\">Ryan Gibb</a> and <a href=\"https://patrick.sirref.org\">Patrick Ferris</a> (who came up with the name) for several design discussions that lead to this post.</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-24 21:43:33",
      "read_at": null,
      "tags": []
    },
    {
      "id": 113349,
      "feed_id": 28,
      "title": "Using Claude in Chrome to navigate out the Cloudflare dashboard",
      "url": "https://simonwillison.net/2025/Dec/22/claude-chrome-cloudflare/#atom-everything",
      "published_at": "2025-12-22 16:10:30",
      "content": "\n    <p>I just had my first success using a browser agent - in this case the <a href=\"https://support.claude.com/en/articles/12012173-getting-started-with-claude-in-chrome\">Claude in Chrome extension</a> - to solve an actual problem.</p>\n<p>A while ago I set things up so anything served from the <code>https://static.simonwillison.net/static/cors-allow/</code> directory of my S3 bucket would have open <code>Access-Control-Allow-Origin: *</code> headers. This is useful for hosting files online that can be loaded into web applications hosted on other domains.</p>\n<p>Problem is I couldn't remember how I did it! I initially thought it was an S3 setting, but it turns out S3 lets you set CORS at the bucket-level but not for individual prefixes.</p>\n<p>I then suspected Cloudflare, but I find the Cloudflare dashboard really difficult to navigate.</p>\n<p>So I decided to give Claude in Chrome a go. I installed and enabled the extension (you then have to click the little puzzle icon and click \"pin\" next to Claude for the icon to appear, I had to ask Claude itself for help figuring that out), signed into Cloudflare, opened the Claude panel and prompted:</p>\n<blockquote>\n<p><code>I'm trying to figure out how come all pages under http://static.simonwillison.net/static/cors/ have an open CORS policy, I think I set that up through Cloudflare but I can't figure out where</code></p>\n</blockquote>\n<p>Off it went. It took it a minute or so but it found exactly what I needed.</p>\n<p><img alt=\"Screenshot of Chrome - on the left is the Cloudflare dashboard. On the right a Claude chat panel which shows the final result: Summary  Found it! Your open CORS policy for the /static/cors/ directory on static.simonwillison.net is configured through Cloudflare Transform Rules, specifically a Response Header Transform Rule named static.simonwillis.net/static/cors-allow/*.  Here's exactly what's set up:  Rule Configuration:      Type: HTTP Response Header Transform Rule     Applies to: Any URL matching https://static.simonwillison.net/static/cors-allow/* (wildcard match)     Action: Modifies the response header access-control-allow-origin and sets it to *  This is why all pages under that path have an open CORS policy — the * value for Access-Control-Allow-Origin means any origin can access those resources.  Where to find/modify it in the dashboard:      Go to your domain settings (simonwillison.net)     Navigate to Rules &gt; Rules Overview (in the left sidebar)     Scroll down to Response Header Transform Rules (should show 2 active)     Click on the rule named static.simonwillis.net/static/cors-allow/* to view or edit it  \" src=\"https://static.simonwillison.net/static/2025/claude-chrome-cloudflare.jpg\" /></p>\n<p>There's no \"share transcript\" option but I used copy and paste and two gnarly Claude Code sessions (<a href=\"https://gistpreview.github.io/?56adf4212345d9967c22aab1362b847b\">one</a>, <a href=\"https://gistpreview.github.io/?1d5f524616bef403cdde4bc92da5b0ba\">two</a>) to turn it into an HTML transcript which <a href=\"https://static.simonwillison.net/static/2025/claude-chrome-transcript.html\">you can take a look at here</a>.</p>\n<p>I remain deeply skeptical of the entire browsing agent category due to my concerns about prompt injection risks—I watched what it was doing here like a <em>hawk</em>—but I have to admit this was a very positive experience.</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/claude\">claude</a>, <a href=\"https://simonwillison.net/tags/browser-agents\">browser-agents</a>, <a href=\"https://simonwillison.net/tags/cors\">cors</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/chrome\">chrome</a>, <a href=\"https://simonwillison.net/tags/cloudflare\">cloudflare</a>, <a href=\"https://simonwillison.net/tags/prompt-injection\">prompt-injection</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-22 17:29:01",
      "read_at": null,
      "tags": []
    },
    {
      "id": 167565,
      "feed_id": 43,
      "title": "AoAH Day 22: Assembling monorepos for agentic OCaml development",
      "url": "https://anil.recoil.org/notes/aoah-2025-22",
      "published_at": "2025-12-22 00:00:00",
      "content": "<p>Over the past three weeks, I've accumulated dozens of OCaml repositories as\npart of this <a href=\"/notes/aoah-2025\">series</a>. Keeping them coordinated has become a real challenge;\nwhen I fix something in one library, dependent packages need updating, and\nagents working on one repo have no visibility into related code.  Ideally,\nI could have all my code in one place and see what agents can do with a lot of local context.</p>\n<p>Today I'm switching tacks to address this with a monorepo workflow built around dune's\n<a href=\"https://www.dra27.uk/blog/platform/2018/08/15/dune-vendoring.html\">excellent vendoring support</a>.\nI last visited this when building <a href=\"/papers/rwo\">RWOv2</a> and its <a href=\"https://github.com/realworldocaml/book\">monorepo</a> when I built a <a href=\"https://github.com/tarides/opam-monorepo\">duniverse</a> tool that turned into the <a href=\"https://github.com/tarides/opam-monorepo\">opam-monorepo</a> plugin that <a href=\"https://mirage.io\">MirageOS</a>\nnow uses. Let's see what happens in today's agentic world instead!</p>\n<p>I also wanted to explore the small group dynamic around vibecoding tools. For today's tool, I first asked <a href=\"https://tarides.com/blog/author/mark-elvers/\">Mark Elvers</a> to spend a few hours sketching out the sort of tool he might want, and then <a href=\"https://jon.recoil.org\">Jon Ludlam</a> has been <a href=\"https://jon.recoil.org/blog/2025/12/claude-and-dune.html\">using Claude</a> to build up <a href=\"https://github.com/ocaml/dune/pull/12995\">complex odocv3 rules</a>. The way we work together with agentic code is quite different from when we've handcrafted a project, with the code itself now being more throwaway as we pass the baton among each other. I'm lightheartedly calling this 'vibrating' amongst each other to reflect the new speed of agentic iterations, and to differentiate from the more thoughtful process of pair programming. Today's tool <strong><a href=\"https://tangled.org/anil.recoil.org/repo-tool\">monopam</a></strong> helps to manage OCaml monorepos for cross-cutting code and documentation.</p>\n<h2 id=\"the-git-repo-coordination-problem\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-git-repo-coordination-problem\"></a>The Git repo coordination problem</h2>\n<p>The OCaml libraries I've built are designed to be standalone, but obviously have interdependencies among each other. <a href=\"/notes/aoah-2025-13\">Requests</a> depends on <a href=\"/notes/aoah-2025-12\">conpool</a> for connection management and HTTP cookie logic from <a href=\"/notes/aoah-2025-11\">cookeio</a>. The codec libraries like <a href=\"/notes/aoah-2025-7\">yamlt</a>, <a href=\"/notes/aoah-2025-18\">tomlt</a>, and <a href=\"/notes/aoah-2025-19\">init</a> all have optional dependencies on bytesrw for serialisation. Meanwhile, <a href=\"/notes/aoah-2025-15\">html5rw</a> depends on <a href=\"/notes/aoah-2025-20\">langdetect</a> and has optional dependencies on the wasm and JavaScript compiler stack.</p>\n<p>Here's the full inventory of what I've built in the last few weeks:</p>\n<div role=\"region\"><table>\n<tr>\n<th>Day</th>\n<th>Library</th>\n<th>Description</th>\n</tr>\n<tr>\n<td>1</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-crockford\">ocaml-crockford</a></td>\n<td>Crockford Base32 encoding</td>\n</tr>\n<tr>\n<td>2</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-jsonfeed\">ocaml-jsonfeed</a></td>\n<td>JSONFeed 1.1 implementation</td>\n</tr>\n<tr>\n<td>3</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/xdge\">xdge</a></td>\n<td>XDG directories with Eio capabilities</td>\n</tr>\n<tr>\n<td>4</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/claudeio\">claudeio</a></td>\n<td>Claude OCaml/Eio SDK</td>\n</tr>\n<tr>\n<td>5</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-bytesrw-eio\">ocaml-bytesrw-eio</a></td>\n<td>Bytesrw/Eio adapter</td>\n</tr>\n<tr>\n<td>6</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-yamlrw\">ocaml-yamlrw</a></td>\n<td>Pure OCaml Yaml 1.2 parser</td>\n</tr>\n<tr>\n<td>7</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-yamlt\">ocaml-yamlt</a></td>\n<td>jsont codecs for Yaml</td>\n</tr>\n<tr>\n<td>8</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/sortal\">sortal</a></td>\n<td>Contacts management CLI</td>\n</tr>\n<tr>\n<td>11</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-punycode\">ocaml-punycode</a></td>\n<td>Punycode RFC3492 implementation</td>\n</tr>\n<tr>\n<td>11</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-publicsuffix\">ocaml-publicsuffix</a></td>\n<td>Public suffix list for cookies</td>\n</tr>\n<tr>\n<td>11</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-cookeio\">ocaml-cookeio</a></td>\n<td>HTTP cookie handling</td>\n</tr>\n<tr>\n<td>12</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-conpool\">ocaml-conpool</a></td>\n<td>TCP/TLS connection pooling</td>\n</tr>\n<tr>\n<td>13</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-requests\">ocaml-requests</a></td>\n<td>HTTP client library</td>\n</tr>\n<tr>\n<td>14</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-karakeep\">ocaml-karakeep</a></td>\n<td>Karakeep bookmark API</td>\n</tr>\n<tr>\n<td>15</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-html5rw\">ocaml-html5rw</a></td>\n<td>HTML5 parser and validator</td>\n</tr>\n<tr>\n<td>16</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-json-pointer\">ocaml-json-pointer</a></td>\n<td>JSON Pointer RFC6901</td>\n</tr>\n<tr>\n<td>16</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/odoc-xo\">odoc-xo</a></td>\n<td>odoc extras for notebooks</td>\n</tr>\n<tr>\n<td>17</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-jmap\">ocaml-jmap</a></td>\n<td>JMAP email client</td>\n</tr>\n<tr>\n<td>18</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-tomlt\">ocaml-tomlt</a></td>\n<td>TOML 1.1 codecs</td>\n</tr>\n<tr>\n<td>19</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-zulip\">ocaml-zulip</a></td>\n<td>Zulip bot framework</td>\n</tr>\n<tr>\n<td>19</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-init\">ocaml-init</a></td>\n<td>INI file codecs</td>\n</tr>\n<tr>\n<td>20</td>\n<td><a href=\"https://tangled.org/anil.recoil.org/ocaml-langdetect\">ocaml-langdetect</a></td>\n<td>Language detection</td>\n</tr>\n</table></div><p>And the Claude skills I've developed along the way:</p>\n<div role=\"region\"><table>\n<tr>\n<th>Skill</th>\n<th>Purpose</th>\n</tr>\n<tr>\n<td><a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-metadata\">claude-ocaml-metadata</a></td>\n<td>Automate opam package setup</td>\n</tr>\n<tr>\n<td><a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-internet-rfc\">claude-ocaml-internet-rfc</a></td>\n<td>Fetch and integrate IETF RFCs</td>\n</tr>\n<tr>\n<td><a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-tidy-code\">claude-ocaml-tidy-code</a></td>\n<td>Refactor generated OCaml</td>\n</tr>\n<tr>\n<td><a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-to-npm\">claude-ocaml-to-npm</a></td>\n<td>Publish js_of_ocaml to NPM</td>\n</tr>\n</table></div><p>So far, I've been publishing each of these as individual Git repositories, but maintaining an <a href=\"https://tangled.org/anil.recoil.org/aoah-opam-repo\">overlay opam repo</a> that a user can add to gain access to consistent metadata that makes the dev packages installable. Unfortunately, incorrect interdependencies are already creeping in; <a href=\"https://github.com/samoht\">Thomas Gazagnaire</a> asked me today why my yamlt library depends on webassembly, and I'm sure it shouldn't -- I've clearly got a stray missing dependency somewhere in my metadata.</p>\n<p>When an agent works on just one repository, it has no visibility into\nhow changes might benefit (or break) dependent code. It also can't make fixes\nfor documentation <em>across</em> repositories. I noticed quite often in the past month that I was\ncloning source packages temporarily into my workspace for the agent to access, and then\ndeleting them.  All this motivates me to investigate alternatives to having lots of small git repos for my day-to-day agentic development.</p>\n<h2 id=\"dunes-vendoring-is-amazing-for-monorepos\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#dunes-vendoring-is-amazing-for-monorepos\"></a>Dune's vendoring is amazing for monorepos</h2>\n<p>Dune has a fantastic but underappreciated feature: it automatically discovers and builds any OCaml code in subdirectories. As <a href=\"https://github.com/dra27\">David Allsopp</a> explained <a href=\"https://www.dra27.uk/blog/platform/2018/08/15/dune-vendoring.html\">back in 2018</a>, you can simply clone dependencies into your project tree and dune will build them together.</p>\n<p>Note that this only works if all the packages contain dune files. Since OCaml is all about choice, there's no hard mandate to use one build tool: it's perfectly fine to use <a href=\"https://github.com/ocaml/ocamlbuild\">ocamlbuild</a> or Makefiles, as long as your libraries install a <a href=\"https://dune.readthedocs.io/en/stable/reference/findlib.html\">findlib META file</a>. Dune will also gain support for <a href=\"https://dune.readthedocs.io/en/latest/explanation/package-management.html\">opam package installation</a> next year to help make this even easier.</p>\n<p>Years ago I built a tool called <a href=\"https://github.com/ocamllabs/duniverse\">duniverse</a> to automate this vendoring workflow. It worked, but required a lot of manual repository management. With agents now doing the heavy lifting, though, I thought it might be easier now and so decided to revisit it.</p>\n<p>Today's work ended up extending <a href=\"https://tarides.com/blog/author/mark-elvers/\">Mark Elvers</a> initial foray into monorepos to release <strong><a href=\"https://tangled.org/anil.recoil.org/repo-tool\">monopam</a></strong>: a little CLI tool that reads opam metadata from a local repository (like <a href=\"https://tangled.org/anil.recoil.org/aoah-opam-repo\">aoah-opam-repo</a>), resolves the dependency graph, materialises the sources as git submodules, and produces a single dune workspace that builds everything together. For now, it depends on an opam local switch to work, but if someone wants to try it with dune package management I'd love to hear how it goes.</p>\n<h2 id=\"materialising-aoah-opam-repo\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#materialising-aoah-opam-repo\"></a>Materialising aoah-opam-repo</h2>\n<p>The <a href=\"https://tangled.org/anil.recoil.org/aoah-opam-repo\">aoah-opam-repo</a> contains all the packages I've built during this series, maintained using the <a href=\"/notes/aoah-2025-5\">opam metadata skill</a>. Let's turn it into a unified source tree using monopam:</p>\n<pre><code class=\"language-bash\">$ monopam --opam-overlay aoah-opam-repo -o aoah-vendor --submodules\nScanning opam overlay at aoah-opam-repo\nFound 21 repositories to process\nInitialized empty Git repository in aoah-vendor/.git/\nUsing git submodules for vendor dependencies\nCloning into ...\nremote: Enumerating objects: 14, done.\nremote: Counting objects: 100% (14/14), done.\nremote: Compressing objects: 100% (11/11), done.\nremote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\n# ...etc\nOutput written to aoah-vendor\n  opam-repository/ - opam package definitions\n  vendor/          - source code\n  setup.sh         - run to pin packages and install deps\n</code></pre>\n<p>This solves the opam constraints, finds a cut of dependencies, and then git\nsubmodule adds the lot of them into my target repository. At this point, we run\n<code>setup.sh</code> which creates an opam local switch and then <code>dune build</code> just works\nusing all the locally cloned repos.</p>\n<pre><code>.\n├── _opam\n├── dune\n├── dune-project\n├── opam-repository\n│   ├── packages\n│   └── repo\n└── vendor\n    ├── dune\n    ├── ocaml-bytesrw-eio\n    ├── ocaml-claudeio\n    ├── ocaml-conpool\n    ├── ocaml-cookeio\n    ├── ocaml-crockford\n    ├── ocaml-html5rw\n    ├── ocaml-init\n    ├── ocaml-json-pointer\n    ├── ocaml-karakeep\n    ├── ocaml-langdetect\n    ├── ocaml-publicsuffix\n    ├── ocaml-punycode\n    ├── ocaml-requests\n    ├── ocaml-tomlt\n    ├── ocaml-yamlrw\n    ├── ocaml-yamlt\n    ├── odoc-xo\n    └── xdge\n</code></pre>\n<p>The directory structure is straightforward: we have our opam repository, a local switch\nand the source code all in one place now, and buildable in a single dune\ninvocation.</p>\n<h2 id=\"cross-cutting-fixes-with-agents\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#cross-cutting-fixes-with-agents\"></a>Cross-cutting fixes with agents</h2>\n<p>With all the code in one place, agents can now spot opportunities that span\nmultiple packages. The first thing I did was to build a full documentation set\nacross all my packages.</p>\n<p><img src=\"/images/aoah-monopam-ss-1.webp\" alt=\"%c\" ></p>\n<h3 id=\"building-unified-documentation-with-odoc3\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#building-unified-documentation-with-odoc3\"></a>Building unified documentation with odoc3</h3>\n<p><a href=\"https://jon.recoil.org\">Jon Ludlam</a> has been doing <a href=\"https://jon.recoil.org/blog/2025/12/claude-and-dune.html\">excellent work</a> on odoc3, the modern documentation generator for OCaml. odoc is a composable documentation generator that has a number of <a href=\"https://ocaml.github.io/odoc/odoc/driver.html\">mini-commands</a> that can be called in sequence to build fragments of HTML. Jon's been adding support into dune build rules to build a fully cross-referenced documentation site across an entire dune workspace.</p>\n<p>This is where the monorepo approach obviously shines, since we could generate a single site for all my code with types linking directly to their definitions across opam packages. The <a href=\"/notes/aoah-2025-16\">interactive notebooks</a> I built earlier could reference any type across the whole codebase.</p>\n<p>I first pinned Jon's odoc branch and then built the unified docs with the right rules.</p>\n<pre><code class=\"language-bash\">$ opam pin add dune https://github.com/jonludlam/dune.git#odoc-v3-rules\n$ dune build @doc\n$ open _build/default/_doc/_html/index.html\n</code></pre>\n<p>This generated a working doc page, that also included cross-referenced links\n<em>across</em> packages. But even more cool is that if a package doesn't exist in the\nlocal monorepo, it also does a best-effort link straight to the central doc\nrepository on OCaml.org.</p>\n<p>There were a few integration issues that may be bugs in the dune rules. For instance:</p>\n<pre><code>&gt; dune build @doc\nFile &quot;/Users/avsm/src/git/knot/aoah-vendor3/_opam/lib/angstrom/META&quot;, line 1, characters 0-0:\nError: Library &quot;angstrom-unix&quot; not found.\n-&gt; required by library &quot;angstrom.unix&quot; in\n   /Users/avsm/src/git/knot/aoah-vendor3/_opam/lib/angstrom\n-&gt; required by alias vendor/ocaml-karakeep/doc\n</code></pre>\n<p>This is a package that's present in the local tree, but not installed in opam.\nAfter I opam installed it, the doc generation worked. This probably shouldn't\nbreak a local docs build, so I commented on the GitHub PR.</p>\n<p>After this, there were genuine bugs in my own documentation, as evidenced by\nwarnings emitted by dune. The agents fixed problems and added cross-references across\nthe documentation, and I could do a single <code>git status</code> to see all the affected\npackages.</p>\n<pre><code>Changes not staged for commit:\n  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)\n  (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory)\n  (commit or discard the untracked or modified content in submodules)\n        modified:   vendor/ocaml-claudeio (modified content)\n        modified:   vendor/ocaml-init (modified content)\n        modified:   vendor/ocaml-json-pointer (modified content)\n        modified:   vendor/ocaml-requests (modified content)\n        modified:   vendor/ocaml-tomlt (modified content)\n        modified:   vendor/ocaml-yamlrw (modified content)\n        modified:   vendor/ocaml-yamlt (modified content)\n        modified:   vendor/xdge (modified content)\n</code></pre>\n<h3 id=\"code-fixing-across-packages\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#code-fixing-across-packages\"></a>Code fixing across packages</h3>\n<p>I then prompted the agents to find opportunities for optimisation <em>across</em>\nall the packages. Running this in a fixpoint ended up allowing for backwards\nand forwards cross-references: for example, it could add &quot;related libraries&quot;\nsections, and also normalise error handling and logging interfaces where\nthere were inconsistencies.</p>\n<p><img src=\"/images/aoah-monopam-ss-2.webp\" alt=\"%c\" title=\"Docs fixes from the agent across repositories\" >\n<img src=\"/images/aoah-monopam-ss-3.webp\" alt=\"%c\" title=\"And similarly, interface fixes work just as well\" ></p>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p>Once I had a consistent monorepo, I could commit the changes and distribute\na batch easily. For example, I uploaded my <a href=\"https://tangled.org/anil.recoil.org/monopam-odocv3-dune-test\">test odocv3 monorepo</a>\nand commented on <a href=\"https://github.com/ocaml/dune/pull/12995\">ocaml/dune#12995</a>.</p>\n<p>On the other hand, monopam's git submodule workflow is awkward to use due to how separate\nsubmodules are from the main git repository. I had to individually commit and push each of the\nchanges, and I couldn't get a unified git diff or make commits <em>across</em> the vendored\nrepositories.  I have a scheme in mind to improve this, which is a topic for tomorrow's post!</p>\n<p>Socially speaking, I'm reasonably convinced a monorepo workflow of <em>some</em> sort is the\nfuture for agentic coding. They just work so much better with local tool calls that can\nrapidly scan a lot of data instead of making remote calls (which are awkward from a permissions\nperspective as well). We'll still need to figure out how the dynamics of 'vibrating' patches\nacross each other goes; it's early days for the dynamics of agentic pair programming.</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-23 23:50:12",
      "read_at": null,
      "tags": []
    },
    {
      "id": 96984,
      "feed_id": 12,
      "title": "Books I Enjoyed in 2025",
      "url": "https://borretti.me/article/books-i-enjoyed-in-2025",
      "published_at": "2025-12-22 00:00:00",
      "content": "A short list.",
      "created_at": "2025-12-22 05:44:43",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92757,
      "feed_id": 28,
      "title": "Quoting Shriram Krishnamurthi",
      "url": "https://simonwillison.net/2025/Dec/21/shriram-krishnamurthi/#atom-everything",
      "published_at": "2025-12-21 05:26:09",
      "content": "\n    <blockquote cite=\"https://parentheticallyspeaking.org/articles/pedagogy-recommendations/\"><p>Every time you are inclined to use the word “teach”, replace it with “learn”. That is, instead of saying, “I teach”, say “They learn”. It’s very easy to determine what you teach; you can just fill slides with text and claim to have taught. Shift your focus to determining how you know whether they learned what you claim to have taught (or indeed anything at all!). That is <em>much</em> harder, but that is also the real objective of any educator.</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://parentheticallyspeaking.org/articles/pedagogy-recommendations/\">Shriram Krishnamurthi</a>, Pedagogy Recommendations</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/teaching\">teaching</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-21 17:16:29",
      "read_at": "2025-12-22 05:45:49",
      "tags": []
    },
    {
      "id": 112090,
      "feed_id": 43,
      "title": "AoAH Day 21: Complete dynamic HTML5 validation in OCaml and the browser",
      "url": "https://anil.recoil.org/notes/aoah-2025-21",
      "published_at": "2025-12-21 00:00:00",
      "content": "<p>With <a href=\"/notes/aoah-2025-20\">language detection</a> now working in OCaml, I completed vibespiling the <a href=\"https://validator.github.io/validator/\">Nu HTML Validator</a> from Java to OCaml. This is the official <a href=\"https://validator.w3.org\">W3C validator</a> used to check HTML5 conformance, and it's a substantial codebase with thousands of validation rules.  I set out to see what a few <em>days</em> of agentic processing would do to transform the complex Java codebase into a more functionally structured pure OCaml codebase.</p>\n<p>The result is a pure OCaml HTML5 conformance checker that integrates with the <a href=\"/notes/aoah-2025-15\">parser</a> I built last week, all published as <strong><a href=\"https://tangled.org/anil.recoil.org/ocaml-html5rw\">ocaml-html5rw</a></strong>. Having the logic in pure OCaml meant that I could <em>also</em> <a href=\"/notes/aoah-2025-20\">compile it</a> into standalone JavaScript and WASM.  Dynamic conformance checking works even better than server-side filtering since live JavaScript executing on the page (and modifying the DOM) can <em>also</em> be checked. I <a href=\"https://www.npmjs.com/package/html5rw-jsoo\">published</a> this to NPM using a <a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-to-npm/blob/main/SKILL.md\">new Claude skill</a>, and coded a live panel overlay to live debug HTML5 issues that I use on my own website now.</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/html5rw-validate/\"> <img src=\"/images/aoah-html5v-ss-1.webp\" alt=\"%c\" title=\"My conformance checker now runs the OCaml straight in the browser on my dev website and highlights errors along with explanations.\" > </a></p>\n<h2 id=\"full-html5-validation-in-ocamljavascriptwasm\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#full-html5-validation-in-ocamljavascriptwasm\"></a>Full HTML5 validation in OCaml/JavaScript/WASM</h2>\n<p>I'm going to talk about my results in reverse today, since I thought the outcomes were so unexpectedly useful.  I took yesterday's <a href=\"/notes/aoah-2025-20\">session</a> and asked Claude to build an <a href=\"https://tangled.org/anil.recoil.org/claude-ocaml-to-npm/blob/main/SKILL.md\">ocaml-to-npm</a> skill, which I used to publish the html5rw JavaScript and wasm <a href=\"https://www.npmjs.com/package/html5rw-jsoo\">to npm</a>.</p>\n<p>This runs the HTML5 validation OCaml code by serialising the live DOM tree and then collecting the various validation errors <em>along with the source</em>. This is sufficient to populate an overlay panel that can not only list the errors, but also highlighting the offending DOM node with a red box. This spotted lots of dynamic errors in my website!</p>\n<p>Publishing on NPM is quite convenient as there are several CDNs that serve the JavaScript directly. I integrate this into the development version of my blog as simply as:</p>\n<pre><code>&lt;script src=&quot;https://cdn.jsdelivr.net/npm/html5rw-jsoo@1.0.0/htmlrw.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\nfunction validateWithPanel() {\n  const result = html5rw.validateAndShowPanel(document.documentElement, {\n    // Annotation options\n    annotation: {\n      addDataAttrs: true,\n      addClasses: true,\n      showTooltips: true,\n      tooltipPosition: 'auto',\n      highlightOnHover: true\n    },\n    // Panel options\n    panel: {\n      initialPosition: 'topRight',\n      draggable: true,\n      collapsible: true,\n      groupBySeverity: true,\n      clickToHighlight: true,\n      showSelectorPath: true,\n      theme: 'auto'\n    }\n});\n&lt;/script&gt;\n</code></pre>\n<p>I'll probably wrap this in a <a href=\"https://webcomponents.org\">webcomponent</a> in the\nfuture as <a href=\"https://github.com/art-w\">Arthur Wendling</a> did with <a href=\"https://github.com/art-w/x-ocaml\">x-ocaml</a> but for\nnow this is already useful on my own website. If anyone has any pointers for\nwhat the right CSS patterns are for adding these debug overlay panels to\nwebsites with minimal intrusion, I'd be grateful. I'm extremely unfamiliar with how\nmodern frontend programming works...</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/html5rw-validate/\"> <img src=\"/images/aoah-html5v-ss-2.webp\" alt=\"%c\" title=\"Did you know that you're not really supposed to have more than one h1 tag? Neither did I...\" > </a></p>\n<p>And of course, if you do prefer to stick to the server-side, then you get fast native code OCaml via a command-line binary provided by the package:</p>\n<pre><code class=\"language-bash\">$ dune exec -- html5check test.html\ntest.html:126.73: error [no-p-element-in-scope]: No “p” element in scope but a\n“p” end tag seen.\ntest.html:113.72: error [missing-alt]: An “img” element must have an “alt”\nattribute, except under certain conditions. For details, consult guidance on\nproviding text alternatives for images.\ntest.html:120.27: error [duplicate-id]: Duplicate ID “duplicate-id”.\ntest.html:123.36: error [disallowed-child]: Element “div” not allowed as child\nof element “span” in this context. (Suppressing further errors from this\nsubtree.)\ntest.html:152.8: info [multiple-h1]: Consider using only one “h1” element per\ndocument (or, if using “h1” elements multiple times is required, consider using\nthe “headingoffset” attribute to indicate that these “h1” elements are not all\ntop-level headings).\n</code></pre>\n<h2 id=\"a-few-days-of-vibespiling\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#a-few-days-of-vibespiling\"></a>A few days of vibespiling</h2>\n<p>The reason this took a few days of background vibespiling comes down to the sheer size of the problem. The <a href=\"https://github.com/validator/validator\">Nu Validator</a> is a mature Java application that is built around Java's <a href=\"https://docs.oracle.com/javase/tutorial/jaxp/sax/parsing.html\">SAX event model</a>, which I last used in 2000 when I worked on <a href=\"https://en.wikipedia.org/wiki/Chello\">Chello's website</a>. Looking through the validator Java code brought back &quot;fond&quot; memories of building <a href=\"https://getyarn.io/yarn-clip/796493b5-d8f6-42fa-9252-3d3803379653\">factories of factory makers</a>.  In the Nu validators, there are lots of rules checkers that iterate through an HTML5 parse tree and extend a base <code>Checker</code> class:</p>\n<pre><code>public final class TableChecker extends Checker {\nprivate Table current;\nprivate final LinkedList&lt;Table&gt; stack = new LinkedList&lt;&gt;();\n\n@Override\npublic void startElement(String uri, String localName,\n                         String qName, Attributes atts)\n  throws SAXException {\n  if (&quot;http://www.w3.org/1999/xhtml&quot;.equals(uri)) {\n    if (&quot;table&quot;.equals(localName)) { push(); } else\n    if (current != null) {\n      if (&quot;td&quot;.equals(localName) || &quot;th&quot;.equals(localName)) {\n        current.cell(atts, localName);\n      }\n      // ... more element handling\n    }\n  } } }\n</code></pre>\n<p>Since the number of rules was massive, a single run of the agent wasn't enough. Instead, I knocked up a <a href=\"/notes/aoah-2025-4\">Claudeio</a> wrapper that ran the agent iteratively exhorting it to continually sample the rules and iterate on a good architecture. This is only possible since the validator has a <a href=\"https://github.com/validator/validator/blob/main/tests/messages.json\">massive test suite</a> with expected outputs. The goal of the agent was therefore to try OCaml code architectures that maximised the number of passing rules, and then combining them towards getting 100% pass rate.</p>\n<p>After a few days, this converged and hit 100% pass rate with a bit of human prompt massaging from me.  The OCaml version replaces inheritance with <a href=\"https://dev.realworldocaml.org/first-class-modules.html\">first-class modules</a> instead, and each checker implements the <a href=\"https://tangled.org/anil.recoil.org/ocaml-html5rw/blob/main/lib/check/checker.mli\">Checker.S</a> signature:</p>\n<pre><code>module type S = sig\n  type state\n  val create : unit -&gt; state\n  val reset : state -&gt; unit\n  val start_element : state -&gt; element:Element.t -&gt; Message_collector.t -&gt; unit\n  val end_element : state -&gt; tag:Tag.element_tag -&gt; Message_collector.t -&gt; unit\n  val characters : state -&gt; string -&gt; Message_collector.t -&gt; unit\n  val end_document : state -&gt; Message_collector.t -&gt; unit\nend\n</code></pre>\n<p>This gives us the same flexibility to compose checkers, but with abstract state\ntypes rather than hidden mutable fields scattered across a class hierarchy.</p>\n<h3 id=\"browsing-the-giant-html5-test-suite\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#browsing-the-giant-html5-test-suite\"></a>Browsing the giant HTML5 test suite</h3>\n<p>I extended the visual HTML test suite generator I built a few days ago to include the thousands of validation tests, and the library now outputs one <a href=\"https://www.cl.cam.ac.uk/~avsm2/html5rw-check/\">epic HTML file</a> that lists each of the thousands of tests.</p>\n<p>Having these tests was essential when doing refactoring, as a small change in one checker affected others. Without the comprehensive test oracle, the agents would quickly diverge out of control.</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/html5rw-check/\"> <img src=\"/images/aoah-html5v-ss-4.webp\" alt=\"%c\" title=\"It's quite fun just browsing around the expect tests to see what's going on with HTML5\" > </a></p>\n<h3 id=\"a-case-study-on-the-table-checker\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#a-case-study-on-the-table-checker\"></a>A case study on the table checker</h3>\n<p>The table checker is one of the more complex validators, tracking cell spans,\ndetecting overlaps, and validating that <code>headers</code> attributes reference valid\n<code>th</code> elements. The Java version spreads this across multiple files, but the\nOCaml version consolidates everything into a <a href=\"https://tangled.org/anil.recoil.org/ocaml-html5rw/blob/main/lib/check/specialized/table_checker.ml\">single module</a> with explicit types.</p>\n<pre><code class=\"language-ocaml\">type cell = {\n  mutable left : int;\n  mutable right : int;\n  mutable bottom : int;\n  headers : string list;\n  element_name : string;\n}\n\ntype row_group = {\n  mutable current_row : int;\n  mutable insertion_point : int;\n  cells_in_effect : ((int * int), cell) Hashtbl.t;\n  mutable cells_on_current_row : cell array;\n  row_group_type : string option;\n}\n\ntype table = {\n  mutable state : table_state;\n  mutable column_count : int;\n  header_ids : (string, unit) Hashtbl.t;\n  cells_with_headers : cell list ref;\n  mutable current_row_group : row_group option;\n  (* ... *)\n}\n</code></pre>\n<p>However, the agent didn't fundamentally change the algorithmic structure; we still have the same basic state machine but with much more succinct variant types.</p>\n<h2 id=\"typed-error-codes\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#typed-error-codes\"></a>Typed error codes</h2>\n<p>One significant quality of life improvement came from refactoring how the error messages are collected for rendering. The Java code uses string formatting throughout to directly output messages, but the OCaml <a href=\"https://tangled.org/anil.recoil.org/ocaml-html5rw/blob/main/lib/check/error_code.mli\">error_code.mli</a> module defines a polymorphic variant hierarchy that's exhaustively checkable:</p>\n<pre><code class=\"language-ocaml\">type table_error = [\n  | `Cell_overlap\n  | `Cell_spans_rowgroup\n  | `Row_no_cells of [`Row of int]\n  | `Column_no_cells of [`Column of int] * [`Elem of string]\n]\n\ntype attr_error = [\n  | `Not_allowed of [`Attr of string] * [`Elem of string]\n  | `Missing of [`Elem of string] * [`Attr of string]\n  | `Bad_value of [`Elem of string] * [`Attr of string] *\n                  [`Value of string] * [`Reason of string]\n  | `Duplicate_id of [`Id of string]\n  (* ... *)\n]\n</code></pre>\n<p>This allows clients to pattern match on specific classes of errors easily:</p>\n<pre><code class=\"language-ocaml\">match err with\n| `Attr (`Duplicate_id (`Id id)) -&gt; handle_duplicate id\n| `Img `Missing_alt -&gt; suggest_alt_text ()\n| `Table `Cell_overlap -&gt; report_overlap ()\n| _ -&gt; default_handler err\n</code></pre>\n<p>This also means we can add new error categories without changing existing code,\nand the compiler tells us if we miss any cases. This is a pretty classic\nusecase for OCaml that both <a href=\"https://github.com/yminsky\">Yaron Minsky</a> and I talked about (in <a href=\"https://www.youtube.com/watch?v=hKcOkWzj0_s\">Caml\nTrading</a> and\n<a href=\"/papers/2010-icfp-xen\">XenServer</a>), but sometimes it's good to remember why I love\nOCaml so much!</p>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p>While the OCaml code generated is by no means sparkling clean, it is useful and\noperational already.  The typed error hierarchy is perhaps the biggest win, as\nit lifts up the abstraction level to be more idiomatic to OCaml style and\neventually makes it easier to perhaps jump over to Haskell or Lean for even\nmore purity and formal specification work.  This is by far the biggest agentic\ncoding translation I've attempted so far, to the point where it used up all my\nClaude 20x Max credits in a matter of days. I have two accounts now!</p>\n<p>What also surprised me was how little the agent struggled with the architectural\ntransformation <em>across programming languages</em>. Given examples of OCaml first\nclass modules (from Real World OCaml and the Jane Street OCaml code), it\nproduced well-structured code.</p>\n<p><img src=\"/images/aoah-html5v-ss-3.webp\" alt=\"%c\" title=\"Claude can introspect its session context to update a skill to match what its learnt\" ></p>\n<p>I still have no idea how I'm going to maintain this code in the long term, but\ndo let me know if the HTML5 checker is useful to you. One little Claude trick\nthat remains handy is that after a prompting session, I prompt the agent to fix\nits own skill based on the feedback its received this session. That helps to\ngeneralise the skills as more projects get to using it.</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-22 16:28:27",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92233,
      "feed_id": 12,
      "title": "Coarse is Better",
      "url": "https://borretti.me/article/coarse-is-better",
      "published_at": "2025-12-21 00:00:00",
      "content": "Make AI weird again.",
      "created_at": "2025-12-21 17:16:21",
      "read_at": null,
      "tags": []
    },
    {
      "id": 94665,
      "feed_id": 43,
      "title": "AoAH Day 20: Human language detection in native code, JS and wasm",
      "url": "https://anil.recoil.org/notes/aoah-2025-20",
      "published_at": "2025-12-20 00:00:00",
      "content": "<p>I took a break from yesterday's <a href=\"\">bot hacking</a> to continue the <a href=\"/notes/aoah-2025-15\">HTML5 parsing</a> in OCaml adventure. Vibespiling seems to have taken off, with <a href=\"https://simonwillison.net/\">Simon Willison</a> <a href=\"https://simonwillison.net/2025/Dec/18/swift-justhtml/\">reporting</a> that there's a <a href=\"https://github.com/kylehowells/swift-justhtml\">Swift version</a> now as well. I got curious about how far I could push the vibespiling support: could we go beyond &quot;just&quot; parsing to also do <em>complete HTML5 validation</em>? The <a href=\"https://validator.github.io/validator/\">Nu HTML Validator</a> is where I went next, which is a bunch of Java code used by the W3C to apply some seriously complex rules for HTML5 validation.</p>\n<p>I decided to split this work into two days, and started with a simple problem: HTML5 validation includes the need for automated language detection to validate that the <code>lang</code> attribute on HTML elements matches the actual content. This is important for accessibility, as screen readers use language hints to select the correct pronunciation.</p>\n<p>The W3C validator uses the <a href=\"https://github.com/shuyo/language-detection\">Cybozu langdetect</a> algorithm, so I vibespiled this into pure OCaml code as <strong><a href=\"https://tangled.org/anil.recoil.org/ocaml-langdetect\">ocaml-langdetect</a></strong>. However, I decided to push harder by compiling this to <em>three</em> different backends: native code OCaml, JavaScript via <a href=\"https://github.com/ocsigen/js_of_ocaml\">js_of_ocaml</a> and then into <a href=\"https://tarides.com/blog/2025-02-19-the-first-wasm-of-ocaml-release-is-out/\">modern WebAssembly using wasm_of_ocaml</a>. As a fun twist, I got the regression tests running as interactive &quot;<a href=\"/notes/aoah-2025-17\">vibesplained</a>&quot; online notebooks that can do language detection in the browser.</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/langdetect-js/langdetect.html\"> <img src=\"/images/aoah-langdetect-ss-1.webp\" alt=\"%c\" title=\"The JavaScript version is interactive so you can test it out directly as you read this post.\" > </a></p>\n<h2 id=\"the-n-gram-frequency-algorithm\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-n-gram-frequency-algorithm\"></a>The n-gram frequency algorithm</h2>\n<p>Language detection via <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-gram analysis</a> is surprisingly simple. The algorithm first trains profiles for each language, by analysing a corpus of text and counting the frequency of sequences of 1-3 characters. This creates a statistical fingerprint of the language.\nThen, when given unknown text it extracts its n-grams and compares against all trained profiles using Bayesian probability. The language whose profile best matches the text wins.</p>\n<p>It turns out that n-gram frequencies are <a href=\"https://web.stanford.edu/~jurafsky/slp3/3.pdf\">remarkably\nstable</a> across different texts\nin the same language. To pick an obvious example, the word &quot;the&quot; appears\nfrequently in English texts, giving bigrams &quot;th&quot; and &quot;he&quot; high frequencies.\nSimilarly, &quot;qu&quot; is common in French, &quot;sch&quot; in German, etc etc.  The algorithm\nuses multiple trials with randomized sampling to avoid overfitting to any\nparticular part of the text. Each trial adjusts the smoothing parameter\nslightly using a Gaussian distribution, all of which should be straightforward\nto implement in OCaml.</p>\n<h2 id=\"implementing-langdetect-in-ocaml\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#implementing-langdetect-in-ocaml\"></a>Implementing langdetect in OCaml</h2>\n<p>I grabbed the <a href=\"https://github.com/validator/validator/tree/main/langdetect\">validator/validator/langdetect</a> directory and vibespiled it from Java to OCaml, which is straightforward now with all the earlier Claude skills I've developed this month. The major hurdle to leap that's different from the other libraries is where to stash the precomputed ngram statistics for all the different languages. I wrote <a href=\"https://github.com/mirage/ocaml-crunch/blob/main/CHANGES.md\">ocaml-crunch</a> for Mirage back in 2011 which just generates OCaml modules, but it's still <a href=\"https://github.com/tarides/hackocaml/issues/21\">surprisingly difficult</a> to be more efficient and store precomputed data. <a href=\"https://www.cst.cam.ac.uk/people/jdy22\">Jeremy Yallop</a> <a href=\"https://discuss.ocaml.org/t/generating-a-map-at-compile-time/16217/7?u=avsm\">noted</a> back in March that his <a href=\"https://www.cl.cam.ac.uk/~jdy22/projects/modular-macros/\">modular macros</a> project should support this sort of usecase but it's not quite ready yet. Similarly, using OCaml Marshal requires stashing the marshalled datastructure somewhere, which is hard to do portably.</p>\n<p>Without a clear optimisation strategy, I prompted the agent to just precompute the profiles directly into OCaml\ncode.  The initial port worked immediately thanks to the clear structure of the\nJava code it was being vibespiled from. The static library was 115MB, but I\ndidn't really notice as the regression tests all passed. The language profiles\ncontain 172,000 unique n-grams across 47 languages, and the naive approach of\ngenerating one OCaml module per language with string literals duplicated\nn-grams across profiles.</p>\n<p>The native code library provides a straightforward interface to query the ngrams via a cmdliner binary:</p>\n<pre><code class=\"language-sh\">$ dune exec langdetect\nHello Thomas Gazagnaire, I'm finally learning French! Just kidding, I don't know anything about it.\nen 1.0000\n\n$ dune exec langdetect\nBonjour Thomas Gazagnaire, j'apprends enfin le français ! Je plaisante, je n'y connais rien.\nfr 1.0000\n\n$ dune exec langdetect\nHello Thomas Gazagnaire, I'm finally learning French! Just kidding, I don't know anything about it.\nBonjour Thomas Gazagnaire, j'apprends enfin le français ! Je plaisante, je n'y connais rien.\nen 0.5714\nfr 0.4286\n</code></pre>\n<h2 id=\"the-115mb-problem-for-javascript\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-115mb-problem-for-javascript\"></a>The 115MB problem for JavaScript</h2>\n<p>But then, when I compiled it to JavaScript using the <a href=\"https://dune.readthedocs.io/en/latest/reference/dune/executable.html#jsoo-field\">dune\nstanzas</a>\nthe massive size was a little too big, with very long compilation times.  The\n<a href=\"https://tangled.org/anil.recoil.org/ocaml-langdetect/commit/69e99a9c342957eee8db079137c803b0895e63fa\">fix</a> was simply to pack everything into a shared data structure across <em>all</em>\nlanguages, looking something like this:</p>\n<pre><code class=\"language-ocaml\">(* Shared string table for all 172K unique n-grams *)\nlet ngram_table = [| &quot;the&quot;; &quot;th&quot;; &quot;he&quot;; ... |]\n\n(* Flat int array: (ngram_index, frequency) pairs for all languages *)\nlet profile_data = [| 0; 15234; 1; 8921; ... |]\n\n(* Offsets: (lang_code, start_index, num_pairs) *)\nlet profile_offsets = [|\n  (&quot;en&quot;, 0, 4521);\n  (&quot;fr&quot;, 9042, 3892);\n  ...\n|]\n</code></pre>\n<p>This reduced the binary from 115MB to around 28MB, which is a reasonable\nreduction without having to resort to compression.  Many n-grams appear in\nmultiple languages (consider Latin alphabet characters) so deduplicating into a\nshared string table eliminated quite a bit of redundancy.</p>\n<p>At this point, I prompted the agent to build me a full Javascript based\nregression test that took the native code, and gave me a browser based\nversion instead.</p>\n<p>One minor hiccup was that the regression tests failed due to JavaScript\nintegers overflowing <em>vs</em> native integers, but the fix was simple and the\nregression tests in the browser made debugging them easy for the agentic loop. Without them, there would have been a lot of human cut-and-pasting which is quite tedious!</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/langdetect-js/langdetect.html\"> <img src=\"/images/aoah-langdetect-ss-2.webp\" alt=\"%c\" title=\"The JavaScript version also executes regression tests in the browser environment derived from the native tests.\" > </a></p>\n<h2 id=\"the-wasm-array-limit\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-wasm-array-limit\"></a>The WASM array limit</h2>\n<p>With the JavaScript size under control, I turned to WASM compilation via <code>wasm_of_ocaml</code>. The first attempt failed with a cryptic error about exceeding operands and a parse error. It turns out WASM's <code>array_new_fixed</code> instruction has a limit of 10000 operands, and our profile data array had 662,000 elements.</p>\n<p>The <a href=\"https://tangled.org/anil.recoil.org/ocaml-langdetect/commit/6f25190ebfb2edc4697b3a2ab05e6b33ae1cba3b\">solution</a> was to chunk the arrays and concatenate at runtime, which incurs runtime overhead but <a href=\"https://github.com/konsoletyper/teavm/issues/971\">is a common enough solution</a>.  The generated code now includes 74 chunks for the profile data and 20 chunks for the n-gram string table, but clocks in at around 20MB and could probably be reduced further with some browser compression.</p>\n<p><a href=\"https://www.cl.cam.ac.uk/~avsm2/langdetect-js/langdetect.html\"> <img src=\"/images/aoah-langdetect-ss-3.webp\" alt=\"%c\" title=\"Wasm mode is effectively the same as JavaScript, but more modern.\" > </a></p>\n<p>Now, the browser tests include the ability to switch between Wasm and\nJavaScript in the same test HTML. There was no real performance difference here, but the dataset is small. The most observable difference is that the wasm needs to be served via a web server and not local filesystem, as otherwise browsers reject it. The browser also must serve <code>.wasm</code> files as mime type <code>application/wasm</code> or it's promptly rejected.</p>\n<h2 id=\"browser-demo\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#browser-demo\"></a>Browser demo</h2>\n<p>The OCaml <code>langdetect-js</code> package provides a browser-ready API using Brr callbacks from HTML to register them with the JavaScript:</p>\n<pre><code>// Detect language\nconst lang = langdetect.detect(&quot;Hello, world!&quot;);  // &quot;en&quot;\n\n// Get probability scores\nconst result = langdetect.detectWithProb(&quot;Bonjour le monde&quot;);\n// { lang: &quot;fr&quot;, prob: 0.9987 }\n\n// Get all candidates\nconst all = langdetect.detectAll(&quot;这是中文&quot;);\n// [{ lang: &quot;zh-cn&quot;, prob: 0.85 }, { lang: &quot;zh-tw&quot;, prob: 0.12 }, ...]\n</code></pre>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p>This was a good intermediate port to work on since it let me exercise Webassembly a bit more, and understand the tradeoffs in OCaml compilation to these other backends.  The process of getting the agent to systematically port first to native code (from Java), and then compile to JavaScript and debug platform-specific issues like the integer overflows, and then go to wasm was quite good.</p>\n<p>The agent was particularly helpful for the tedious work of generating the\nchunked array code and debugging the Unicode normalization edge cases.</p>\n<p>For future hacking, there are several language optimisations coming up in <a href=\"https://oxcaml.org\">OxCaml</a> that should make this even more efficient; support for compile time metaprogramming (so I could for example compute a perfect hash statically for all the ngrams), and also for smaller integer sizes so I dont need to use a full 31-bit range for the ngram values. However, I couldn't quite get the wasm_of_ocaml constraints on the oxcaml branch working so I ran out of time today to get this going. Package management takes me out of the flow zone yet again!</p>\n<p>Now that langdetect works, we'll go onto the full HTML5 validator in <a href=\"\">Day 21</a>!</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-21 19:10:06",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92758,
      "feed_id": 28,
      "title": "Quoting Andrej Karpathy",
      "url": "https://simonwillison.net/2025/Dec/19/andrej-karpathy/#atom-everything",
      "published_at": "2025-12-19 23:07:52",
      "content": "\n    <blockquote cite=\"https://karpathy.bearblog.dev/year-in-review-2025/\"><p>In 2025, Reinforcement Learning from Verifiable Rewards (RLVR) emerged as the de facto new major stage to add to this mix. By training LLMs against automatically verifiable rewards across a number of environments (e.g. think math/code puzzles), the LLMs spontaneously develop strategies that look like \"reasoning\" to humans - they learn to break down problem solving into intermediate calculations and they learn a number of problem solving strategies for going back and forth to figure things out (see DeepSeek R1 paper for examples).</p></blockquote>\n<p class=\"cite\">&mdash; <a href=\"https://karpathy.bearblog.dev/year-in-review-2025/\">Andrej Karpathy</a>, 2025 LLM Year in Review</p>\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/andrej-karpathy\">andrej-karpathy</a>, <a href=\"https://simonwillison.net/tags/llm\">llm</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llm-reasoning\">llm-reasoning</a>, <a href=\"https://simonwillison.net/tags/definitions\">definitions</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/deepseek\">deepseek</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-21 17:16:29",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92759,
      "feed_id": 28,
      "title": "Sam Rose explains how LLMs work with a visual essay",
      "url": "https://simonwillison.net/2025/Dec/19/sam-rose-llms/#atom-everything",
      "published_at": "2025-12-19 18:33:41",
      "content": "\n    \n<p><strong><a href=\"https://ngrok.com/blog/prompt-caching/\">Sam Rose explains how LLMs work with a visual essay</a></strong></p>\nSam Rose is one of my favorite authors of <a href=\"https://simonwillison.net/tags/explorables/\">explorable interactive explanations</a> - here's <a href=\"https://samwho.dev/\">his previous collection</a>.</p>\n<p>Sam joined ngrok in September as a developer educator. Here's his first big visual explainer for them, ostensibly about how prompt caching works but it quickly expands to cover tokenization, embeddings, and the basics of the transformer architecture.</p>\n<p>The result is one of the clearest and most accessible introductions to LLM internals I've seen anywhere.</p>\n<div style=\"text-align: center\"><img alt=\"Animation. Starts in tokens mode with an array of 75, 305, 24, 887 - clicking embeddings animates those into a 2D array showing each one to be composed of three floating point numbers.\" src=\"https://static.simonwillison.net/static/2025/tokens-embeddings.gif\" style=\"max-width: 100%\"></div>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/explorables\">explorables</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/sam-rose\">sam-rose</a>, <a href=\"https://simonwillison.net/tags/tokenization\">tokenization</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-21 17:16:29",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92343,
      "feed_id": 18,
      "title": "This month at Tailscale for December 2025",
      "url": "https://tailscale.com/blog/december-25-product-update",
      "published_at": "2025-12-19 14:00:00",
      "content": "What's new in Tailscale clients this month, including a number of fixes, changes, and updates to our latest features.",
      "author": "Kevin Purdy",
      "created_at": "2025-12-21 17:16:23",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92760,
      "feed_id": 28,
      "title": "Introducing GPT-5.2-Codex",
      "url": "https://simonwillison.net/2025/Dec/19/introducing-gpt-52-codex/#atom-everything",
      "published_at": "2025-12-19 05:21:17",
      "content": "\n    \n<p><strong><a href=\"https://openai.com/index/introducing-gpt-5-2-codex/\">Introducing GPT-5.2-Codex</a></strong></p>\nThe latest in OpenAI's <a href=\"https://simonwillison.net/tags/gpt-codex/\">Codex family of models</a> (not the same thing as their Codex CLI or Codex Cloud coding agent tools).</p>\n<blockquote>\n<p>GPT‑5.2-Codex is a version of <a href=\"https://openai.com/index/introducing-gpt-5-2/\">GPT‑5.2⁠</a> further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.</p>\n</blockquote>\n<p>As with some previous Codex models this one is available via their Codex coding agents now and will be coming to the API \"in the coming weeks\". Unlike previous models there's a new invite-only preview process for vetted cybersecurity professionals for \"more permissive models\".</p>\n<p>I've been very impressed recently with GPT 5.2's ability to <a href=\"https://simonwillison.net/2025/Dec/15/porting-justhtml/\">tackle multi-hour agentic coding challenges</a>. 5.2 Codex scores 64% on the Terminal-Bench 2.0 benchmark that GPT-5.2 scored 62.2% on. I'm not sure how concrete that 1.8% improvement will be!</p>\n<p>I didn't hack API access together this time (see <a href=\"https://simonwillison.net/2025/Nov/9/gpt-5-codex-mini/\">previous attempts</a>), instead opting to just ask Codex CLI to \"Generate an SVG of a pelican riding a bicycle\" while running the new model (effort medium). <a href=\"https://tools.simonwillison.net/codex-timeline?url=https://gist.githubusercontent.com/simonw/10ad81e82889a97a7d28827e0ea6d768/raw/d749473b37d86d519b4c3fa0892b5e54b5941b38/rollout-2025-12-18T16-09-10-019b33f0-6111-7840-89b0-aedf755a6e10.jsonl#tz=local&amp;q=&amp;type=all&amp;payload=all&amp;role=all&amp;hide=1&amp;truncate=1&amp;sel=3\">Here's the transcript</a> in my new Codex CLI timeline viewer, and here's the pelican it drew:</p>\n<p><img alt=\"Alt text by GPT-5.2-Codex: A minimalist illustration of a white pelican with a large orange beak riding a teal bicycle across a sandy strip of ground. The pelican leans forward as if pedaling, its wings tucked back and legs reaching toward the pedals. Simple gray motion lines trail behind it, and a pale yellow sun sits in the top‑right against a warm beige sky.\" src=\"https://static.simonwillison.net/static/2025/5.2-codex-pelican.png\" />\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/codex-cli\">codex-cli</a>, <a href=\"https://simonwillison.net/tags/gpt-codex\">gpt-codex</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-21 17:16:29",
      "read_at": null,
      "tags": []
    },
    {
      "id": 92761,
      "feed_id": 28,
      "title": "Agent Skills",
      "url": "https://simonwillison.net/2025/Dec/19/agent-skills/#atom-everything",
      "published_at": "2025-12-19 01:09:18",
      "content": "\n    \n<p><strong><a href=\"https://agentskills.io/\">Agent Skills</a></strong></p>\nAnthropic have turned their <a href=\"https://simonwillison.net/tags/skills/\">skills mechanism</a> into an \"open standard\", which I guess means it lives in an independent <a href=\"https://github.com/agentskills/agentskills\">agentskills/agentskills</a> GitHub repository now? I wouldn't be surprised to see this end up <a href=\"https://simonwillison.net/2025/Dec/9/agentic-ai-foundation/\">in the AAIF</a>, recently the new home of the MCP specification.</p>\n<p>The specification itself lives at <a href=\"https://agentskills.io/specification\">agentskills.io/specification</a>, published from <a href=\"https://github.com/agentskills/agentskills/blob/main/docs/specification.mdx\">docs/specification.mdx</a> in the repo.</p>\n<p>It is a deliciously tiny specification - you can read the entire thing in just a few minutes. It's also quite heavily under-specified - for example, there's a <code>metadata</code> field described like this:</p>\n<blockquote>\n<p>Clients can use this to store additional properties not defined by the Agent Skills spec</p>\n<p>We recommend making your key names reasonably unique to avoid accidental conflicts</p>\n</blockquote>\n<p>And an <code>allowed-skills</code> field:</p>\n<blockquote>\n<p>Experimental. Support for this field may vary between agent implementations</p>\n<p>Example:</p>\n<pre><code>allowed-tools: Bash(git:*) Bash(jq:*) Read\n</code></pre>\n</blockquote>\n<p>The Agent Skills homepage promotes adoption by OpenCode, Cursor,Amp, Letta, goose, GitHub, and VS Code. Notably absent is OpenAI, who are <a href=\"https://simonwillison.net/2025/Dec/12/openai-skills/\">quietly tinkering with skills</a> but don't appear to have formally announced their support just yet.</p>\n<p><strong>Update 20th December 2025</strong>: OpenAI <a href=\"https://developers.openai.com/codex/skills/\">have added Skills to the Codex documentation</a> and the Codex logo is now <a href=\"https://agentskills.io/\">featured on the Agent Skills homepage</a> (as of <a href=\"https://github.com/agentskills/agentskills/commit/75287b28fb7a8106d7798de99e13189f7bea5ca0\">this commit</a>.)\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/anthropic\">anthropic</a>, <a href=\"https://simonwillison.net/tags/ai-agents\">ai-agents</a>, <a href=\"https://simonwillison.net/tags/coding-agents\">coding-agents</a>, <a href=\"https://simonwillison.net/tags/skills\">skills</a></p>\n\n\n\n",
      "author": "Simon Willison",
      "created_at": "2025-12-21 17:16:29",
      "read_at": null,
      "tags": []
    },
    {
      "id": 93082,
      "feed_id": 43,
      "title": "AoAH Day 19: Zulip bot framework to bring Vicuna the friendly camel back",
      "url": "https://anil.recoil.org/notes/aoah-2025-19",
      "published_at": "2025-12-19 00:00:00",
      "content": "<p>After building <a href=\"/notes/aoah-2025-18\">tomlt</a> yesterday for TOML 1.1 parsing, I proceeded to integrate it with my group's <a href=\"https://zulip.com\">Zulip</a> chat <a href=\"https://eeg.zulipchat.com\">server</a>. I then discovered that Zulip actually uses Python's <a href=\"https://docs.python.org/3/library/configparser.html\">configparser</a> INI format for its <code>.zuliprc</code> files rather than TOML, woops! But this gave me the perfect opportunity to attempt to quickly replicate the tomlt experience with a <em>third</em> config format codec library for Windows-style INI files as well.</p>\n<p>So today I released both <strong><a href=\"https://tangled.org/anil.recoil.org/ocaml-zulip\">ocaml-zulip</a></strong> for Zulip API integration and <strong><a href=\"https://tangled.org/anil.recoil.org/ocaml-init\">ocaml-init</a></strong> for INI file codecs that are compatible with Pythonic features such as variable interpolation. Along the way, I developed a new regression test mechanism by writing a Zulip bot that tests the Zulip API using OCaml Zulip!</p>\n<h2 id=\"zulip-organized-chat-for-distributed-teams\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#zulip-organized-chat-for-distributed-teams\"></a>Zulip: Organized chat for distributed teams</h2>\n<p>Zulip is an open-source &quot;async first&quot; messaging app that strikes a nice balance between immediate and thoughtful conversations:</p>\n<blockquote>\n<ul>\n<li>In Zulip, channels determine who gets a message. Each conversation within a channel is labeled with a topic, which keeps everything organized.</li>\n<li>You can read Zulip one conversation at a time, seeing each message in context, no matter how many other conversations are going on.</li>\n<li>If anything is out of place, it's easy to move messages, rename and split topics, or even move a topic to a different channel\n<cite>-- <a href=\"https://zulip.com/why-zulip/\">Why Zulip?</a>, 2024</cite></li>\n</ul>\n</blockquote>\n<p>Zulip itself is fully open source and has a pretty straightforward REST API to communicate with the server, and so I deployed my <a href=\"/notes/aoah-2025-13\">requests library</a> as well the various API codecs to interface with it. I used the <a href=\"https://github.com/zulip/python-zulip-api\">Zulip Python SDK</a> and the <a href=\"https://github.com/zulip/zulip-js\">Zulip JavaScript</a> library to give me two API specifications. Unlike previous libraries I've vibecoded, there's no language-agnostic test suite so I needed to get a bit more creative to verify correctness by building a live bot to test itself.</p>\n<h3 id=\"the-zulip-ini-config-format\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-zulip-ini-config-format\"></a>The Zulip INI config format</h3>\n<p>Zulip's <code>.zuliprc</code> file looks like this:</p>\n<pre><code>[api]\nemail = bot@example.com\nkey = your-api-key-here\nsite = https://your-domain.zulipchat.com\n</code></pre>\n<p>This is classic INI format as used by Python's configparser module. It's\nsimpler than TOML but isn't fully compatible as it has quirks like case-insensitive\nkeys, multiline value support via continuation lines, and basic variable\ninterpolation with a <code>%(name)s</code> syntax.</p>\n<p>I couldn't find a feature complete implementation of Python's module, so I quickly\nreused the <a href=\"/notes/aoah-2025-18\">tomlt</a> approach to build <a href=\"https://tangled.org/anil.recoil.org/ocaml-init\">ocaml-init</a> with bidirectional\ncodecs. The resulting API is unsurprisingly extremely similar to manipulate this format file:</p>\n<pre><code class=\"language-ocaml\">type server_config = { host : string; port : int; debug : bool }\n\nlet server_codec = Init.Section.(\n  obj (fun host port debug -&gt; { host; port; debug })\n  |&gt; mem &quot;host&quot; Init.string ~enc:(fun c -&gt; c.host)\n  |&gt; mem &quot;port&quot; Init.int ~enc:(fun c -&gt; c.port)\n  |&gt; mem &quot;debug&quot; Init.bool ~dec_absent:false ~enc:(fun c -&gt; c.debug)\n  |&gt; finish\n)\n</code></pre>\n<p>There's a <code>bool</code> codec in this library that follows Python's configparser\nsemantics exactly, accepting yes/no/true/false/on/off/1/0 as boolean values.\nThis was important for compatibility with existing Zulip configuration files.</p>\n<h2 id=\"the-zulip-bot-framework\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#the-zulip-bot-framework\"></a>The Zulip bot framework</h2>\n<p>With configuration parsing sorted, I turned to building the actual bot\nframework. Our research group at <a href=\"https://eeg.zulipchat.com\">eeg.zulipchat.com</a>\nhas been wanting an Atom feed bot to post updates from our blogs' Atom/RSS sources,\nso this seemed like a good excuse to knock up a bot.</p>\n<p>I prompted the agent to follow the basic <a href=\"https://zulip.com/api/deploying-bots\">Python\nbotserver</a> considerations but adapted to\na more Eio and OCaml idiomatic style. This resulted in a nice design where a\nbot handler is just a function:</p>\n<pre><code class=\"language-ocaml\">type handler =\n  storage:Storage.t -&gt; identity:identity -&gt;\n  Message.t -&gt; Response.t\n</code></pre>\n<p>The Zulip library provides modules for <code>storage</code> for persisting state (on the Zulip server side), <code>identity</code> containing functions to access the bot's email and user ID, and the incoming <code>Message.t</code>. The handler returns a <code>Response.t</code> which can be a reply in the same context (DM or channel/topic), or a post to a specific channel, or a direct message, or an indication that the bot's ignoring the event.</p>\n<p>There's an echo bot handler that's executable that shows the API quite simply:</p>\n<pre><code class=\"language-ocaml\">let echo_handler ~storage ~identity msg =\n  let bot_email = identity.Bot.email in\n  let sender_email = Message.sender_email msg in\n\n  (* Ignore our own messages *)\n  if sender_email = bot_email then Response.silent\n  else\n    (* Remove bot mention and echo back *)\n    let cleaned_msg = Message.strip_mention msg ~user_email:bot_email in\n    if cleaned_msg = &quot;&quot; then\n      Response.reply (Printf.sprintf &quot;Hello %s!&quot; (Message.sender_full_name msg))\n    else\n      Response.reply (Printf.sprintf &quot;Echo: %s&quot; cleaned_msg)\n</code></pre>\n<p>After this running the bot in an Eio environment is a single function call:</p>\n<pre><code class=\"language-ocaml\">let () =\n  Eio_main.run @@ fun env -&gt;\n  Eio.Switch.run @@ fun sw -&gt;\n  let config = Zulip_bot.Config.load ~fs:(Eio.Stdenv.fs env) &quot;echo-bot&quot; in\n  Zulip_bot.Bot.run ~sw ~env ~config ~handler:echo_handler\n</code></pre>\n<h2 id=\"tying-it-all-together-with-requests\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#tying-it-all-together-with-requests\"></a>Tying it all together with Requests</h2>\n<p>One nice payoff from this <a href=\"/notes/aoah-2025\">advent</a> series is seeing how\nthe libraries begin to compose. The Zulip OCaml package depends on\n<a href=\"/notes/aoah-2025-13\">Requests</a> for HTTPS communication back to the server:</p>\n<pre><code class=\"language-ocaml\">let create ~sw env auth =\n  let session =\n    Requests.create ~sw\n      ~default_headers:(Requests.Headers.of_list [\n        (&quot;Authorization&quot;, Auth.to_basic_auth_header auth);\n        (&quot;User-Agent&quot;, &quot;OCaml-Zulip/1.0&quot;);\n      ])\n      ~follow_redirects:true\n      ~verify_tls:true\n      env\n  in\n  { auth; session }\n</code></pre>\n<p>This shows how the session abstraction in Requests can persist the common auth\ntokens required, making subsequent API calls very syntactically succinct.  The\nbot framework also uses <a href=\"/notes/aoah-2025-3\">XDGe</a> for configuration directory\nresolution, <a href=\"/notes/aoah-2025-2\">jsont</a> for JSON parsing of API responses, and of\ncourse Eio throughout for async operations. The dependency graph is starting to\nlook like actual infrastructure!</p>\n<h2 id=\"testing-with-a-regression-bot\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#testing-with-a-regression-bot\"></a>Testing with a regression bot</h2>\n<p>Remember that problem I mentioned earlier about Zulip lacking a\nlanguage-agnostic test suite? My cunning solution was recursive; let's just\nbuild a Zulip bot that can test itself!\nI built a <a href=\"https://tangled.org/anil.recoil.org/ocaml-zulip/blob/main/examples/regression_test.ml\">regression test bot</a> that exercises as\nmuch of the Zulip API as possible when triggered via a direct message:</p>\n<pre><code class=\"language-ocaml\">let make_handler ~env ~channel =\n  fun ~storage ~identity:_ msg -&gt;\n    let content = String.lowercase_ascii (Message.content msg) in\n    let sender_email = Message.sender_email msg in\n    (* Only respond to DMs containing &quot;regress&quot; *)\n    if Message.is_private msg &amp;&amp; String.starts_with ~prefix:&quot;regress&quot; content\n    then (\n      let client = Storage.client storage in\n      let summary = run_tests ~env ~client ~channel ~trigger_user:sender_email in\n      Response.reply summary)\n    else Response.silent\n</code></pre>\n<p>When someone sends the bot a DM with &quot;regress&quot;, it runs through a comprehensive\ntest suite covering user operations, channel management, message\nsending/editing, reactions, message flags, typing indicators, presence updates,\nand alert words.  I duly started the harness and DMed Vicuna, and the bot\nimmediately spat out a number of failures resulting from errors in the codecs.\nHowever, as the logs show, the errors also included useful information about\nwhere the protocol decoding had gone wrong.</p>\n<p><img src=\"/images/aoah-zulip-regress-1.webp\" alt=\"%c\" title=\"The first run of my Zulip regression bot\" ></p>\n<p>The bot posts a summary to a test channel showing which tests passed or failed,\ncomplete with timing information. This turned out to be more useful than\ntraditional unit tests since it exercises the actual API against a real Zulip\nserver. After one round of fixes, the bot successfully posted its results to\na Zulip channel recording success!</p>\n<p><img src=\"/images/aoah-zulip-regress-2.webp\" alt=\"%c\" title=\"The bot posts the results of its Zulip regression test to Zulip!\" ></p>\n<h2 id=\"composable-command-line-terms\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#composable-command-line-terms\"></a>Composable command-line terms</h2>\n<p>One pattern I've been developing across these libraries is to expose <a href=\"https://github.com/dbuenzli/cmdliner\">cmdliner</a>\nterms that compose together to make it easy to build CLI tools that expose\nthe configuration needed by a library along with a manual page.</p>\n<p>The <code>Zulip_bot.Cmd</code> module provides a <code>config_term</code> that bundles common bot configuration parameters:</p>\n<pre><code class=\"language-ocaml\">let config_term default_name env =\n  let fs = env#fs in\n  Term.(const (fun name config_file verbosity verbose_http -&gt;\n        setup_logging ~verbose_http:verbose_http.value verbosity.value;\n        load_config ~fs ~name ~config_file)\n    $ name_term default_name\n    $ config_file_term\n    $ verbosity_term\n    $ verbose_http_term default_name)\n</code></pre>\n<p>While this looks complex, all it's doing is to combine various declarations\nof command-line parameters.  This combines individual terms for the bot name, config file path, verbosity level,\nand HTTP-level debugging into a single composable unit.</p>\n<p>The <code>verbose_http_term</code> controls logging sources in the <a href=\"/notes/aoah-2025-13\">Requests</a>\nlibrary, letting you toggle detailed HTTP traces without that being the default verbose output.</p>\n<pre><code class=\"language-ocaml\">let verbose_http_term app_name =\n  let env_name = String.uppercase_ascii app_name ^ &quot;_VERBOSE_HTTP&quot; in\n  let env_info = Cmdliner.Cmd.Env.info env_name in\n  Arg.(value &amp; flag &amp; info [ &quot;verbose-http&quot; ] ~env:env_info ~doc)\n</code></pre>\n<p>Each bot can then define its command with minimal boilerplate:</p>\n<pre><code class=\"language-ocaml\">let bot_cmd eio_env =\n  let info = Cmd.info &quot;echo_bot&quot; ~version:&quot;2.0.0&quot; ~doc ~man in\n  let config_term = Zulip_bot.Cmd.config_term &quot;echo-bot&quot; eio_env in\n  Cmd.v info Term.(const (fun config -&gt; run_echo_bot config eio_env) $ config_term)\n</code></pre>\n<p>The source tracking also helps with debugging by showing where each value\noriginated from -- whether from the command line, environment variables,\nXDG config files, or defaults. This makes it much easier to understand\nwhy a bot is behaving a certain way when deployed.</p>\n<p><img src=\"/images/aoah-zulip-regress-3.webp\" alt=\"%c\" title=\"The manual pages for a bot are pretty good by default thanks to the cmdliner terms.\" ></p>\n<h2 id=\"reflections\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#reflections\"></a>Reflections</h2>\n<p>It's nice to get to the Zulip bot framework at last, since this is one of the\nthings I wanted to fix at the start of the month. It uses a number of things\nI've built this month, including the Requests library to handle HTTP, the INI\ncodec for Python configuration, XDG to handle path resolution, and so on. Each\npiece is small and focused, and generatively replicated from other\n(human-written) exemplar libraries from the OCaml ecosystem.</p>\n<p>The only &quot;agentic trick&quot; I learnt today was the value of live debugging, as I\nfound with both <a href=\"/notes/aoah-2025-17\">JMAP email</a> and <a href=\"/notes/aoah-2025-14\">Karakeep</a>. Building\nservices amenable to this kind of live mocking is something I'll keep in mind\nfor the future. It's also extremely useful to have good terminal manual pages,\nsince those can also be interrogated by coding agents as well as be used by humans.</p>\n",
      "author": "Anil Madhavapeddy",
      "created_at": "2025-12-21 17:16:36",
      "read_at": null,
      "tags": []
    }
  ],
  "page": 1,
  "per_page": 20,
  "total": 1621,
  "total_pages": 82
}
